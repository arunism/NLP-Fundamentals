{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Natural Language ToolKit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NLTK` is used for statistical natural language processing. It consists of module called `tokenize` with several methods that aids in splitting text to tokens like:\n",
    "\n",
    "- word_tokenize\n",
    "- sent_tokenize\n",
    "- wordpunct_tokenize\n",
    "- WhitespaceTokenizer\n",
    "- TreebankWordTokenizer\n",
    "- TweetTokenizer\n",
    "- MWETokenizer\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./../assets/tokenization/nltk.jpg\"><br>\n",
    "    <a href=\"https://udemy.com/course/python-for-data-science-and-machine-learning-bootcamp\"><i>[Image source]</i></a>\n",
    "</p>\n",
    "\n",
    "As an example here, we‚Äôll be using same text for all the tokenization modules mentioned here in this repo. Let's start by installing nltk library with the following command:\n",
    "\n",
    "`pip3 install nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "#Turn off the pretty print\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Good muffins cost $3.88. Please buy me two of them.\\n\\nThanks.üôÇüòç\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punkt is designed to learn parameters (a list of abbreviations, etc.) unsupervised from a corpus similar to the target domain. Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries and sometimes sentence can start with non-capitalized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### word_tokenize\n",
    "\n",
    "`word_tokenize` can be used to find the words and punctuations in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$', '3.88', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks.üôÇüòç']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### sent_tokenize\n",
    "\n",
    "`sent_tokenize` operates at the level of sentences and generates the tokens as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88.', 'Please buy me two of them.', 'Thanks.üôÇüòç']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokens = sent_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### wordpunct_tokenize\n",
    "\n",
    "`wordpunct_tokenize` splits text on whitespace and punctuation. This particular tokenizer requires the `Punkt` sentence tokenization models to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$', '3', '.', '88', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.üôÇüòç']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "tokens = wordpunct_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### WhitespaceTokenizer\n",
    "\n",
    "`WhitespaceTokenizer` extracts tokens from string based on whitespaces, new lines and tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$3.88.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.üôÇüòç']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreebankWordTokenizer\n",
    "\n",
    "This tokenizer incorporates a variety of common rules for english word tokenization. It separates phrase-terminating punctuation like (?!.;,) from adjacent tokens and retains decimal numbers as a single token. Besides, it contains rules for English contractions. You can find all the rules for the Treebank Tokenizer [here](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.treebank).\n",
    "\n",
    "For example ‚Äúdon‚Äôt‚Äù is tokenized as [‚Äúdo‚Äù, ‚Äún‚Äôt‚Äù].\n",
    "\n",
    "[Source](https://neptune.ai/blog/tokenization-in-nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$', '3.88.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.üôÇüòç']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweetTokenizer\n",
    "\n",
    "TweetTokenizer is a rule based tokenizer specially desined for text data like tweets. It allows splitting of emojis into different words which can be helpful for certain tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$', '3.88', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'üôÇ', 'üòç']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWETokenizer\n",
    "\n",
    "MWETokenizer stands for Multi-word Expression Tokenizer. It provides a function `add_mwe()` that allows the user to enter multiple word expressions before using the tokenizer on the text data. More simply, it can merge multi-word expressions into single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$', '3.88', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks.üôÇüòç']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "tokenizer = MWETokenizer()\n",
    "tokens = tokenizer.tokenize(word_tokenize(text))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$_3.88_.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks.üôÇüòç']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('$', '3.88', '.'))\n",
    "tokens = tokenizer.tokenize(word_tokenize(text))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
