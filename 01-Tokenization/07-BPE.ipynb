{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Byte-Pair Encoding, is a data compression algorithm that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte.\n",
    "\n",
    "`For example:` Let 'aaabdaaabac' be the string. Here 'aa' is the most frequent pair of bytes, so we replace it with some unused byte, let's say 'Z'. Now the string becomes 'ZabdZabac'. 'ab' is now the most frequent pair, we replace it with 'Y' and get 'ZYdZYac'. After the latest update the most frequent pair be 'ZY'. Replace it with 'X' so the obtained string be 'XdXac'.\n",
    "\n",
    "To adapt this idea for word segmentation, instead of replacing frequent pair of bytes, we now merge subword pairs that frequently occur. Let's elaborate it stepwise:\n",
    "\n",
    "- We initialize the vocabulary representing each word as a sequence of characters, plus a special end-of-word symbol '/w', which allows us to restore the original tokenization after translation. More on this below.\n",
    "- Next, we iteratively count all symbol pairs and merge each occurrence of the most frequent pair (a, b) into ab. Each merge operation produces a new symbol which represents a character n-gram.\n",
    "- We'll keep repeating the previous process until the target vocabulary size is reached.\n",
    "\n",
    "Why is end of word symbol '/w' important? Let's say there is a token 'est'. Both the words 'eastern' and 'smallest' contain the subword 'est'. However, meanings of the subword between the two are quite different. With the end of word symbol, if there is a token est/w, the model immediately interprets the subword represents the terminating meaning of the word.\n",
    "\n",
    "![](./../assets/tokenization/bpe.jpg)\n",
    "*[[Image Source]](https://www.computer.org/csdl/journal/tb/2020/05/08678449/1nJsrGwiJqg)*\n",
    "\n",
    "We will be implementing Byte-Pair Encoding using Google's SentencePiece library. Let's get started by installing the library.\n",
    "\n",
    "`pip3 install sentencepiece`\n",
    "\n",
    "`NOTE:` BPE Tokenization can be implemented using HuggingFace's tokenizers library as done for WordPiece and Unigram Tokenization. You may refer to `wordpiece.ipynb` or `unigram.ipynb` for similar implementation of BPE Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`Remember:`\n",
    "- To download the dataset and copy it to the correct location.\n",
    "- Create a 'models' directory inside the root directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training and loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code allows you to import '.py' file from one directory behind (i.e. root directory)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=./../data/shakespeare.txt --model_type=bpe --model_prefix=./../models/bpe --vocab_size=1000 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./../data/shakespeare.txt\n",
      "  input_format: \n",
      "  model_prefix: ./../models/bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: ./../data/shakespeare.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 6794 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=287458\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9569% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=70\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999569\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 6794 sentences.\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 6794\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 10860\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6296 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1588 size=20 all=1472 active=1401 piece=it\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1034 size=40 all=2004 active=1933 piece=ot\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=630 size=60 all=2728 active=2657 piece=ld\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=478 size=80 all=3158 active=3087 piece=▁for\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=392 size=100 all=3433 active=3362 piece=▁your\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=390 min_freq=23\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=327 size=120 all=4065 active=1631 piece=ver\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=252 size=140 all=4376 active=1942 piece=hich\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=219 size=160 all=4764 active=2330 piece=▁'\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=201 size=180 all=5109 active=2675 piece=lf\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=179 size=200 all=5404 active=2970 piece=▁by\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=177 min_freq=22\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=156 size=220 all=5593 active=1190 piece=ind\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=140 size=240 all=5895 active=1492 piece=ong\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=123 size=260 all=6015 active=1612 piece=▁their\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=113 size=280 all=6205 active=1802 piece=ING\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=105 size=300 all=6404 active=2001 piece=▁them\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=104 min_freq=19\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=100 size=320 all=6596 active=1190 piece=▁give\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=94 size=340 all=6777 active=1371 piece=all\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=90 size=360 all=6921 active=1515 piece=red\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=84 size=380 all=7076 active=1670 piece=ig\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=78 size=400 all=7266 active=1860 piece=▁yet\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=78 min_freq=17\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=72 size=420 all=7401 active=1136 piece=▁live\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=67 size=440 all=7498 active=1233 piece=▁honour\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=63 size=460 all=7677 active=1412 piece=▁ab\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=59 size=480 all=7845 active=1580 piece=▁(\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=57 size=500 all=7989 active=1724 piece=▁look\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=57 min_freq=16\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=54 size=520 all=8063 active=1072 piece=li\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=52 size=540 all=8212 active=1221 piece=▁int\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=48 size=560 all=8280 active=1289 piece=ment\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=46 size=580 all=8415 active=1424 piece=tun\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=44 size=600 all=8519 active=1528 piece=▁ro\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=44 min_freq=14\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=42 size=620 all=8597 active=1065 piece=ign\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=41 size=640 all=8698 active=1166 piece=▁wife\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=38 size=660 all=8850 active=1318 piece=OB\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=37 size=680 all=8891 active=1359 piece=ier\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=36 size=700 all=9065 active=1533 piece=me\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=36 min_freq=12\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=34 size=720 all=9178 active=1103 piece=rum\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=33 size=740 all=9261 active=1186 piece=▁Now\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32 size=760 all=9326 active=1251 piece=orrow\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=31 size=780 all=9370 active=1295 piece=▁done\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30 size=800 all=9446 active=1371 piece=▁dost\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=30 min_freq=11\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28 size=820 all=9502 active=1057 piece=ory\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=27 size=840 all=9599 active=1154 piece=len\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=27 size=860 all=9641 active=1196 piece=▁strong\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26 size=880 all=9711 active=1266 piece=▁happ\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=25 size=900 all=9788 active=1343 piece=▁ob\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=25 min_freq=10\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=25 size=920 all=9848 active=1053 piece=▁forth\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: ./../models/bpe.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: ./../models/bpe.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config\n",
    "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n",
    "\n",
    "params = ' '.join([\n",
    "    f'--input=./../{config.DATA_PATH}',\n",
    "    '--model_type=bpe',\n",
    "    f'--model_prefix=./../{config.MODEL_PATH}/bpe',\n",
    "    f'--vocab_size={config.VOCAB_SIZE}',\n",
    "    '--pad_id=0',\n",
    "    '--unk_id=1',\n",
    "    '--bos_id=2',\n",
    "    '--eos_id=3'\n",
    "])\n",
    "SentencePieceTrainer.train(params)\n",
    "\n",
    "sp = SentencePieceProcessor()\n",
    "sp.load(f'./../{config.MODEL_PATH}/bpe.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Showcasing Subword Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Good muffins cost $3.88. Please buy me two of them.\\n\\nThanks.🙂😍\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens: 1000\n"
     ]
    }
   ],
   "source": [
    "print('Number of Unique Tokens: {}'.format(sp.get_piece_size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁G', 'ood', '▁mu', 'ff', 'in', 's', '▁c', 'ost', '▁', '$', '3', '.', '88', '.', '▁P', 'le', 'ase', '▁b', 'u', 'y', '▁me', '▁two', '▁of', '▁them', '.', '▁Than', 'ks', '.', '🙂😍']\n",
      "Good muffins cost $3.88. Please buy me two of them. Thanks.🙂😍\n"
     ]
    }
   ],
   "source": [
    "encoded_pieces = sp.encode_as_pieces(text)\n",
    "print(encoded_pieces)\n",
    "\n",
    "decoded_pieces = sp.decode_pieces(encoded_pieces)\n",
    "print(decoded_pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Showcasing Subword Operations with Numeric Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197, 172, 217, 750, 9, 936, 30, 260, 930, 1, 990, 949, 1, 949, 116, 77, 468, 14, 942, 944, 82, 714, 41, 303, 949, 781, 647, 949, 1]\n",
      "Good muffins cost  ⁇ 3. ⁇ . Please buy me two of them. Thanks. ⁇ \n"
     ]
    }
   ],
   "source": [
    "encoded_ids = sp.encode_as_ids(text)\n",
    "print(encoded_ids)\n",
    "\n",
    "decoded_ids = sp.decode_ids(encoded_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "▁The\n"
     ]
    }
   ],
   "source": [
    "piece_id = sp.piece_to_id('▁The')\n",
    "print(piece_id)\n",
    "\n",
    "piece = sp.id_to_piece(piece_id)\n",
    "print(piece)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a9e37ffee3a5a10c5d687ae20c51ee3d4ba310e8eddff89fa7a028272d595a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
