{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In Spacy, the process of tokenizing a text into segments of words and punctuation is done in various steps. It processes the text from left to right.\n",
    "\n",
    "- First, the tokenizer split the text on whitespace similar to the split() function.\n",
    "- Then the tokenizer checks whether the substring matches the tokenizer exception rules. For example, ‚Äúdon‚Äôt‚Äù does not contain whitespace, but should be split into two tokens, ‚Äúdo‚Äù and ‚Äún‚Äôt‚Äù, while ‚ÄúU.K.‚Äù should always remain one token.\n",
    "- Next, it checks for a prefix, suffix, or infix in a substring, these include commas, periods, hyphens, or quotes. If it matches, the substring is split into two tokens.\n",
    "\n",
    "    <p align=\"center\">\n",
    "      <img src=\"./../assets/tokenization/spacy.jpg\"><br>\n",
    "      <a href=\"https://machinelearningknowledge.ai/complete-guide-to-spacy-tokenizer-with-examples/\"><i>[Source]</i></a>\n",
    "    </p>\n",
    "\n",
    "The text used here is same as that used for other tokenizers modules. This is done to maintain the uniformity throughout the demo. Let's begin with the installation of spacy library:\n",
    "\n",
    "`pip3 install spacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the model of your desired language (English in this case), create an object for the loaded model and then iterate over the tokens of the object.\n",
    "\n",
    "Remember to download the language model before creating an object of the model. You can download one from the below models as per your requirement:\n",
    "\n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download en_core_web_md\n",
    "python3 -m spacy download en_core_web_lg\n",
    "```\n",
    "\n",
    "or use this command to download them all:\n",
    "\n",
    "`python3 -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Good muffins cost $3.88. Please buy me two of them.\\n\\nThanks.üôÇüòç\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', '\\n\\n', 'Thanks', '.', 'üôÇ', 'üòç']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokens = nlp(text)\n",
    "\n",
    "print([token.text for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining Tokenizer with Default Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', '\\n\\n', 'Thanks', '.', 'üôÇ', 'üòç']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "tokenizer = nlp.tokenizer\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "print([token.text for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining Tokenizer for Words in English Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88.', 'Please', 'buy', 'me', 'two', 'of', 'them.', '\\n\\n', 'Thanks.üôÇüòç']\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "print([token.text for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining Special Rules\n",
    "\n",
    "In some cases, we need to customize tokenization rules. For example: We are breaking a `\\n\\n` token into two separate `\\n` tokens. This is easily achievable by adding special rules to the tokenizer object and with the help of `ORTH` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', '\\n', '\\n', 'Thanks', '.', 'üôÇ', 'üòç']\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "special_case = [{ORTH: '\\n'}, {ORTH: '\\n'}]\n",
    "nlp.tokenizer.add_special_case('\\n\\n', special_case)\n",
    "\n",
    "tokens = nlp(text)\n",
    "print([token.text for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Customizing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "special_cases = {'\\n\\n': [{ORTH: '\\n\\n'}]}\n",
    "prefix_re = re.compile(r'''^[\\[\\(\"']''')\n",
    "suffix_re = re.compile(r'''[\\]\\)\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88.', 'Please', 'buy', 'me', 'two', 'of', 'them.', '\\n\\n', 'Thanks.üôÇüòç']\n"
     ]
    }
   ],
   "source": [
    "nlp.tokenizer = Tokenizer(\n",
    "    nlp.vocab,\n",
    "    rules = special_cases,\n",
    "    prefix_search = prefix_re.search,\n",
    "    suffix_search = suffix_re.search,\n",
    "    infix_finditer = infix_re.finditer,\n",
    "    url_match = simple_url_re.match\n",
    ")\n",
    "\n",
    "tokens = nlp(text)\n",
    "print([token.text for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a9e37ffee3a5a10c5d687ae20c51ee3d4ba310e8eddff89fa7a028272d595a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
