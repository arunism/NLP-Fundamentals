{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Unigram Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Unlike BPE (which is a frequency-based model), Unigram is a probability based model. We start with a large vocabulary and keep decreasing its size gradually until we reach the desired size. How do we get the base vocabulary? There are multiple ways to do so. Let's pass by a couple of ways here:\n",
    "\n",
    "- Apply BPE on the initial corpus\n",
    "- Consider the most common substrings in pre-tokenized words\n",
    "\n",
    "We compute a loss function at each iteration of training. The token that results in the least increase in loss is considered 'least important' and hence removed. Due to financial reasons we don't just remove a single character, but a set of *'p'* characters (*p* being a hyperparameter representing a ratio of characters contributing to the lowest increase in loss).\n",
    "\n",
    "`Why the name Unigram?` This is because unigram is the language model that considers each token to be independent of the tokens before it.\n",
    "\n",
    "![](./../assets/tokenization/wordpiece.jpg)\n",
    "\n",
    "We will be implementing WordPiece tokenization using HuggingFace's `tokenizers` library as done for wordpiece tokenizer. Let's get started by installing the library (if not installed already).\n",
    "\n",
    "`pip3 install tokenizers`\n",
    "\n",
    "`NOTE:` Unigram Tokenization can be implemented using Google's SentencePiece library as done for Byte-Pair Encoding. You may refer to `bpe.ipynb` for similar implementation of Unigram Tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code allows you to import '.py' file from one directory behind (i.e. root directory)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import config\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unk_token = \"<UNK>\"\n",
    "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]\n",
    "\n",
    "tokenizer = Tokenizer(Unigram())\n",
    "trainer = UnigramTrainer(unk_token=unk_token, special_tokens=spl_tokens)\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_files = ['./../{}'.format(config.DATA_PATH)]\n",
    "model_path = './../{}/unigram.json'.format(config.MODEL_PATH)\n",
    "\n",
    "tokenizer.train(data_files, trainer)\n",
    "tokenizer.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenize Input String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Good muffins cost $3.88. Please buy me two of them.\\n\\nThanks.üôÇüòç\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[207, 457, 21, 1854, 19, 5, 1580, 0, 324, 6, 449, 449, 6, 90, 1226, 713, 12, 42, 372, 18, 202, 6, 1738, 5, 6, 0]\n",
      "['G', 'ood', 'm', 'uff', 'in', 's', 'cost', '$', '3', '.', '8', '8', '.', 'P', 'lease', 'bu', 'y', 'me', 'two', 'of', 'them', '.', 'Thank', 's', '.', 'üôÇüòç']\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(text)\n",
    "\n",
    "print(output.ids)\n",
    "print(output.tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a9e37ffee3a5a10c5d687ae20c51ee3d4ba310e8eddff89fa7a028272d595a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
