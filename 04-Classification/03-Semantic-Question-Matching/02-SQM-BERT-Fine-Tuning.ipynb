{"cells":[{"cell_type":"markdown","metadata":{"id":"-QcRfbp69o80"},"source":["# BERT Fine Tuning for Semantic Question Matching"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1677497100879,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"1H5JdoNj9ujq"},"outputs":[],"source":["# !pip install transformers\n","# !pip install torchdata"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4814,"status":"ok","timestamp":1677497105675,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"isxZl1j69ums"},"outputs":[],"source":["import os\n","import time\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","# import multiprocessing\n","from torch.optim import Adam\n","from torchtext.datasets import QQP\n","# from torch.nn import functional as F\n","from torch.utils.data import DataLoader\n","from torch.nn.utils import clip_grad_norm_\n","from transformers import BertTokenizerFast\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data.dataset import random_split\n","from transformers import BertForSequenceClassification\n","from torchtext.data.functional import to_map_style_dataset"]},{"cell_type":"markdown","metadata":{"id":"vU7CElZbKDzv"},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1677497105677,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"Uf5v4tF39upQ"},"outputs":[],"source":["data_iter = QQP()\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8184,"status":"ok","timestamp":1677497113849,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"o0l2EuC_9usM"},"outputs":[],"source":["data = to_map_style_dataset(data_iter)\n","split_ratio = int(len(data) * 0.95)\n","data, test_data = random_split(data, [split_ratio, len(data) - split_ratio])\n","split_ratio = int(len(data) * 0.95)\n","train_data, valid_data = random_split(data, [split_ratio, len(data) - split_ratio])"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1925,"status":"ok","timestamp":1677497115762,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"fHGJ7-V69u4A"},"outputs":[],"source":["EPOCHS = 10\n","LR = 2e-5\n","BATCH_SIZE = 64\n","NUM_CLASSES = len(set([label for (label, _, _) in train_data]))\n","MAX_LEN = 100\n","EMBED_SIZE = 64\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1677497115764,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"8kyLuQ9G9u6U"},"outputs":[],"source":["def collate_fn(batch): \n","  labels, input_ids, attention_mask = [], [], []\n","  for _label, _q1, _q2 in batch:\n","    encoded_dict = tokenizer.encode_plus(\n","        _q1, _q2, padding='max_length', max_length=MAX_LEN, truncation=True\n","    )\n","    labels.append(_label)\n","    input_ids.append(encoded_dict['input_ids'])\n","    # token_type_ids.append(encoded_dict['token_type_ids'])\n","    attention_mask.append(encoded_dict['attention_mask'])\n","  \n","  labels = torch.tensor(labels).to(device)\n","  input_ids = torch.tensor(input_ids).to(device)\n","  # token_type_ids = torch.tensor(token_type_ids).to(device)\n","  attention_mask = torch.tensor(attention_mask).to(device)\n","  return labels, input_ids, attention_mask"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1677497115765,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"S5dG5hmN9u89"},"outputs":[],"source":["# n_cores = multiprocessing.cpu_count()\n","\n","train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n","valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n","test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"]},{"cell_type":"markdown","metadata":{"id":"EXdrKcr7VEpS"},"source":["### Model Definition"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7865,"status":"ok","timestamp":1677497123615,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"BZQO0ryi9u_i","outputId":"69719a07-ccf2-47dd-e18a-8cc016926fb5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","model = model.to(device)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1677497123618,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"VN9RrTQM9vE8"},"outputs":[],"source":["# criterion = nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=LR)"]},{"cell_type":"markdown","metadata":{"id":"phyHCyZVWK_K"},"source":["### Model Training"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1677497123619,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"sxubMQkX9vIQ"},"outputs":[],"source":["def train(dataloader):\n","  model.train()\n","  total_loss, total_acc, total_count = 0, 0, 0\n","  for labels, input_ids, attention_mask in dataloader:\n","    optimizer.zero_grad()\n","    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","    loss = outputs.loss\n","    loss.backward()\n","    clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","    total_count += 1\n","  return total_loss/total_count"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1677497123621,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"},"user_tz":-345},"id":"VE-CwIEaWXE7"},"outputs":[],"source":["def evaluate(dataloader):\n","  model.eval()\n","  total_loss, total_acc, total_count = 0, 0, 0\n","  for labels, input_ids, attention_mask in dataloader:\n","    with torch.no_grad():\n","      loss, logits = model(input_ids, attention_mask=attention_mask, labels=labels)\n","    y_pred = np.argmax(logits.detach().numpy(), axis=1).flatten()\n","    total_loss += loss.item()\n","    total_acc += (y_pred == labels).sum().item()\n","    total_count += 1\n","  return total_loss/total_count, total_acc/total_count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pX3A6RfgWXHo"},"outputs":[],"source":["if not os.path.exists('./../models'):\n","  os.mkdir('./../models')\n","\n","best_valid_loss = float('inf')\n","for epoch in range(EPOCHS):\n","  start_time = time.time()\n","  train_loss = train(train_dataloader)\n","  val_loss, val_acc = evaluate(valid_dataloader)\n","  if val_loss < best_valid_loss:\n","    best_valid_loss = val_loss\n","    torch.save(model.state_dict(), './../models/sqm-bert.pt')\n","  print(f'Epoch: {epoch+1:02} | Time: {time.time()-start_time} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')"]},{"cell_type":"markdown","metadata":{"id":"rjOo-EaYWkXn"},"source":["### Model Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SpvSYuP9vKn"},"outputs":[],"source":["test_loss, test_acc = evaluate(test_dataloader)\n","print(f'Test Loss: {test_loss:.3f} | Test Accuracy: {test_acc:.3f}')"]},{"cell_type":"markdown","metadata":{"id":"-rUusV2K9vgI"},"source":["### References\n","\n","- [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n","- [Training and fine-tuning](https://huggingface.co/transformers/v3.3.1/training.html)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"eb733a8caf0f7db04cd8f7f9b68381322b723c3440fb2793d8721e65b7e12d04"}}},"nbformat":4,"nbformat_minor":0}
