# Semantic Question Matching

Semantic question matching is the task of identifying whether two questions have similar meanings. There are various techniques for semantic question matching, including:

1. **Word2Vec:** A neural network model that learns vector representations of words, which can be used to measure the similarity of questions based on their word embeddings.
2. **Siamese Networks:** A neural network architecture that learns to compare two inputs and output a similarity score, which can be used to measure the similarity of questions.
3. **Sequence-to-Sequence Models:** A type of neural network architecture that learns to map one sequence to another, which can be used to translate one question into another and measure their similarity.
4. **Latent Semantic Analysis (Cosine Similarity):** A statistical technique that involves representing each question as a bag-of-words and projecting them into a lower-dimensional space, where their similarity can be measured using cosine similarity.

## Includes

- [ ] [Semantic Question Matching with Cosine Similarity](https://github.com/arunism/NLP-Fundamentals/blob/master/04-Classification/03-Semantic-Question-Matching/01-SQM-Cosine-Similarity.ipynb)
- [x] [BERT Fine Tuning for Semantic Question Matching](https://github.com/arunism/NLP-Fundamentals/blob/master/04-Classification/03-Semantic-Question-Matching/02-SQM-BERT-Fine-Tuning.ipynb)
- [ ] [Semantic Question Matching with Siamese Network](https://github.com/arunism/NLP-Fundamentals/blob/master/04-Classification/03-Semantic-Question-Matching/03-SQM-Siamese-Network.ipynb)

## References

Please check the bottom of each notebook for references. Each notebook consists of respective references.
