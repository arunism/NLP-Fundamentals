{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ileuCT4AsYIF"
   },
   "source": [
    "# Sentiment Analysis Improved\n",
    "\n",
    "We have already understood the fundamentals of sentiment analysis from the previous notebook. What are we including in this notebook to improvise our result?\n",
    "\n",
    "- Pack padded sequences\n",
    "- Bidirectional and Multi-Layer RNN\n",
    "- Regularization (dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPRvPYQFuy0z"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1676126892219,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "SmYKneFRsYIN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "from torchtext import data\n",
    "from torch.optim import Adam\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1676126892223,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "MQ59lX5su4VC"
   },
   "outputs": [],
   "source": [
    "# !pip install torchdata\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1676126892227,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "2FYa7rWRvt9G"
   },
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = IMDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGiIM5gwu9pl"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "The preprocessing step is same as the previous one. You can further add to the preprocessing step. Check the following techniques to work more with preprocessing:\n",
    "\n",
    "- Apply minimum word occurance for vocabulary building\n",
    "- Word stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 33638,
     "status": "ok",
     "timestamp": 1676126925838,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "QE_rekjNu4WC"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "  text = re.sub(r\"[-()\\\"#/@;:<>{}=~|.?,]\", \" \", text)\n",
    "  text = re.sub(' +', ' ', text)\n",
    "  return text.lower().split()\n",
    "\n",
    "\n",
    "tokens = list()\n",
    "for _, line in train_iterator:\n",
    "  tokens += tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1676126925839,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "KX5_KEyuu4Yd"
   },
   "outputs": [],
   "source": [
    "vocab = {'<PAD>':0, '<UNK>':1}\n",
    "for i, word in enumerate(set(tokens), start=len(vocab)):\n",
    "  vocab[word] = i\n",
    "\n",
    "\n",
    "def word_to_index(text):\n",
    "  return [vocab[word] if word in vocab.keys() else vocab['<UNK>'] for word in tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1676126925839,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "hpDLzu7Uu4a-"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def collate_fn(batch):\n",
    "  labels, text, text_len = [], [], []\n",
    "  for label, line in batch:\n",
    "    labels.append(label)\n",
    "    text.append(torch.tensor(word_to_index(line)))\n",
    "    text_len.append(len(line))\n",
    "  text = pad_sequence(text, padding_value=vocab['<PAD>'])\n",
    "  labels = torch.tensor(labels).to(device)\n",
    "  text = torch.tensor(text).to(device)\n",
    "  text_len = torch.tensor(text_len).to(device)\n",
    "  return labels, text, text_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYeBjbADxCwf"
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "The improvised model contains a little more techniques implemented. We have already very frequently discussed about bidirectional and multi-layered rnn in our previous notebooks. Let's give a quick sight over what is regularization.\n",
    "\n",
    "**Regularization:** Addition of improvising techniques to our model adds additional parameters to it. The more the number of parameters, more is the probability of our model to overfit. It is because more parameters memorize the information from training data, causing very low training error but poor generalization to unseen/test data). Regularization technique helps us combat this issue. We will be using `dropout` as a regularization technique in our case. Though a model with dropped parameters is a weaker model, it works because the predictions from all these weaker models when averaged together behave as an ensemble of weaker models that give better result.\n",
    "\n",
    "There are `n` hidden states returned by the model with shape `[num_layers*num_directions, batch_size, hid_dim]`, ordered as `[forward_0, backward_0, ..., forward_n, backward_n]`. As we only need the last layer (both forward and backward), we will slice the hidden state using `hidden[-2,:,:]` for forward_n and `[-1,:,:]` for backward_n and then concatenate them together. Likewise we multiplied hidden_dim by two i.e. `hidden_dim*2` while initializing linear layer because we have two states for each hidden layer (forward and backward).\n",
    "\n",
    "`Note:` Length of pack_padded_sequence must be a CPU tensor so we explicitly use `.to('cpu')` to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1676126925840,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "3aLboywRu4dV"
   },
   "outputs": [],
   "source": [
    "class SentimentAnalysis(nn.Module):\n",
    "  def __init__(self, input_dim, embed_dim, hid_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "    self.rnn = nn.LSTM(embed_dim, hid_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "    self.fc = nn.Linear(hid_dim*2, output_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, text, text_len): # [seq_len, batch_size]\n",
    "    embedded = self.dropout(self.embedding(text)) # [seq_len, batch_size, embed_dim]\n",
    "    packed_embedded = pack_padded_sequence(embedded, text_len.to('cpu'), enforce_sorted=False) # Pack sequence\n",
    "    packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "    # Unpack sequence  # Output over padding tokens are zero tensors\n",
    "    output, output_len = pad_packed_sequence(packed_output)\n",
    "    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "    return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1676126925840,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "ToWhW4V20tz-"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() # Convert to float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmQo46UWy6l3"
   },
   "source": [
    "## Parameter Handling\n",
    "\n",
    "Here we will be defining parameters, feed them to our sentiment analysis model and check the count of trainable parameters using `count_parameters` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1676126925840,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "ceK64JGrzdha"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 128\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = SentimentAnalysis(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1676126925841,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "DztT-cRF0T2V",
    "outputId": "ee33a671-1113-400a-dd14-aa5f65088c3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,198,441 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzxFkw7C3OeV"
   },
   "source": [
    "## Model Training\n",
    "\n",
    "## Training Model\n",
    "\n",
    "`train_dataloader` and `test_dataloader` create the batches of dataset for train and test respectively. The `count_parameters` function is used to calculate the number of trainable parameters. `train` function is used to train each epoch of the model and so is the `evaluate` function to test or evaluate the trained model. `epoch_time` returns the execution time for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1676126926711,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "2SO2cwW03Eo3"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_iterator, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_iterator, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1676126926714,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "Jq6-SUye34tC"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1676126926716,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "Z8h5Thts4Egt"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "  epoch_loss = 0\n",
    "  epoch_accuracy = 0\n",
    "  batch_idx = 0\n",
    "  model.train()\n",
    "  for labels, text, text_len in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(text, text_len).squeeze(1)\n",
    "    loss = criterion(predictions, labels.float())\n",
    "    accuracy = binary_accuracy(predictions, labels.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss\n",
    "    epoch_accuracy += accuracy\n",
    "    batch_idx += 1\n",
    "  return epoch_loss/batch_idx, epoch_accuracy/batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1676126926717,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "fqnyd0X44G5M"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "  epoch_loss = 0\n",
    "  epoch_accuracy = 0\n",
    "  batch_idx = 0\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for labels, text, text_len in dataloader:\n",
    "      predictions = model(text, text_len).squeeze(1)\n",
    "      loss = criterion(predictions, labels.float())\n",
    "      accuracy = binary_accuracy(predictions, labels.float())\n",
    "      epoch_loss += loss\n",
    "      epoch_accuracy += accuracy\n",
    "      batch_idx += 1\n",
    "  return epoch_loss/batch_idx, epoch_accuracy/batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1676126926719,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "EMIajELA4JO2"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1676126926722,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "T8SoLd6O4Lgs"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "if not os.path.exists('./../models'):\n",
    "  os.mkdir('./../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 2158,
     "status": "error",
     "timestamp": 1676126928852,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "9WRZ0GMK4LjB",
    "outputId": "34319bbd-bf20-4da7-ab81-49f745fa5f63"
   },
   "outputs": [],
   "source": [
    "best_train_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "  start_time = time.time()\n",
    "  train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
    "  end_time = time.time()\n",
    "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "  if train_loss < best_train_loss:\n",
    "      best_train_loss = train_loss\n",
    "      torch.save(model.state_dict(), './../models/sentimet-basic.pt')\n",
    "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}:{epoch_secs} | Train Accuracy: {train_acc:.3f} | Train Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1676126928853,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "1ovVkOMs4Sb5"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "print(f'Test Accuracy: {test_acc:.3f} | Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFQOYlGs49F7"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Inference is the process of running live data points into a machine learning algorithm (or “ML model”) to calculate an output such as a single numerical score. This process is also referred to as “operationalizing a machine learning model” or “putting a machine learning model into production.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1676126928854,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "KLgy0qZ948og"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "  model.eval()\n",
    "  indexed_tokens = [vocab.get(word, vocab['<UNK>']) for word in tokenize(sentence)]\n",
    "  tensors = torch.LongTensor(indexed_tokens).to(device).unsqueeze(1)\n",
    "  tensor_len = torch.LongTensor([len(indexed_tokens)])\n",
    "  prediction = torch.sigmoid(model(tensors, tensor_len))\n",
    "  return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1676126928854,
     "user": {
      "displayName": "Arun Ghimire",
      "userId": "12379322823635086573"
     },
     "user_tz": -345
    },
    "id": "SXTPhNCKJrhn"
   },
   "outputs": [],
   "source": [
    "# Example of a Negative Review\n",
    "print(predict_sentiment(model, \"The movie was terrible!\"))\n",
    "\n",
    "# Example of a Positive Review\n",
    "print(predict_sentiment(model, \"The movie was amazing!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_csKMCg4Z7k"
   },
   "source": [
    "## References\n",
    "\n",
    "- [IMDB Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "- [PyTorch Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
