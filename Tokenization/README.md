# Tokenization

Tokenization is one of the first steps in NLP pipeline.
It is a technique to split a sentence, phrase, paragraph or an entire document to smaller units.
These smaller units are called tokens. Tokens must not always be words. They can be either
symbols and letters (., !, a, z, ...) or sentences or any other forms of text.

> The program works well with python version >=3.8.0.
> Make sure the requirements mentioned here are satisfied,
> or the result may not be as expected.

## Methods Included

- [NLTK](https://github.com/arunism/Deep-Dive-to-NLP/blob/master/Tokenization/nltk.ipynb)
    
    This module is used for statistical natural language processing. It consists of module called `tokenize`
    with several methods that aids in splitting text to tokens like: `word_tokenize`, `sent_tokenize`,
    `wordpunct_tokenize`, `WhitespaceTokenizer`.

    <p align="center">
        <img src="./assets/nltk.jpg"><br/>
        <a href="https://udemy.com/course/python-for-data-science-and-machine-learning-bootcamp"><em>Image source<em/><a/>
    <p/>
