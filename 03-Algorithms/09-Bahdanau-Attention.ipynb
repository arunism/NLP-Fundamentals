{"cells":[{"cell_type":"markdown","metadata":{"id":"qbANObzZHMwa"},"source":["# Bahdanau Attention (Additive Attention)\n","\n","One of the motivations behind Bahdanau Attention approach was the use of a fixed-length context vector in the basic encoderâ€“decoder approach. This limitation makes the basic encoder-decoder approach to underperform with long sentences. In basic encoder-decoder approach, the last element of a sequence contains the memory of all the previous elements and thus form a fixed-dimension context vector. But in case of Bahdanau attention approach:\n","\n","- First, we initialize the Decoder states by using the last states of the Encoder as usual\n","- Then at each decoding time step:\n","    - We use Encoder's all hidden states and the previous Decoder's output to calculate a Context Vector by applying an Attention Mechanism\n","    - Lastly, we concatenate the Context Vector with the previous Decoder's output to create the input to the decoder.\n","\n","All the preprocessing steps will be same as that used in seq2seq model. Let's start by doing the same."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"DebHB7L1HMwl","executionInfo":{"status":"ok","timestamp":1674916645480,"user_tz":-345,"elapsed":1527,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"outputs":[],"source":["import os\n","import time\n","import math\n","import torch\n","import random\n","import torch.nn as nn\n","from torch.optim import Adam\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","from typing import Iterable, List\n","from torch.utils.data import DataLoader\n","from torchtext.datasets import Multi30k\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator as bvfi"]},{"cell_type":"markdown","source":["## Tokenization and Vocabulary Building"],"metadata":{"id":"-pWXR-b3IQxJ"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"weFs2tN-HMws","executionInfo":{"status":"ok","timestamp":1674916645481,"user_tz":-345,"elapsed":6,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"outputs":[],"source":["SRC_LANG = 'de'\n","TGT_LANG = 'en'\n","specials = {'<UNK>': 0, '<PAD>': 1, '<SOS>': 2, '<EOS>': 3}\n","\n","tokenizer = dict()\n","vocab = dict()"]},{"cell_type":"markdown","source":["Create source and target language tokenizer. Make sure to install the dependencies.\n","\n","```\n","pip install -U torchdata\n","pip install -U spacy\n","python -m spacy download en_core_web_sm\n","python -m spacy download de_core_news_sm\n","```"],"metadata":{"id":"URLIdPHNIaqh"}},{"cell_type":"code","source":["# !pip install -U torchdata\n","# !pip install -U spacy\n","# !python -m spacy download en_core_web_sm\n","# !python -m spacy download de_core_news_sm"],"metadata":{"id":"YyHv9O2IIgwx","executionInfo":{"status":"ok","timestamp":1674916645482,"user_tz":-345,"elapsed":6,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ofk_KjkFHMwy","executionInfo":{"status":"ok","timestamp":1674916657485,"user_tz":-345,"elapsed":11316,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"outputs":[],"source":["tokenizer[SRC_LANG] = get_tokenizer('spacy', language='de_core_news_sm')\n","tokenizer[TGT_LANG] = get_tokenizer('spacy', language='en_core_web_sm')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"d2Xx5E7bHMw1","executionInfo":{"status":"ok","timestamp":1674916657487,"user_tz":-345,"elapsed":35,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"outputs":[],"source":["def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n","    language_index = {SRC_LANG: 0, TGT_LANG: 1}\n","\n","    for data_sample in data_iter:\n","        yield tokenizer[language](data_sample[language_index[language]])"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_uzQBvm1HMw3","executionInfo":{"status":"ok","timestamp":1674916661664,"user_tz":-345,"elapsed":4210,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"outputs":[],"source":["for lang in [SRC_LANG, TGT_LANG]:\n","    train_iterator, valid_iterator, test_iterator = Multi30k()    # Training data Iterator\n","    vocab[lang] = bvfi(yield_tokens(train_iterator, lang), min_freq=1, specials=specials.keys(), special_first=True)"]},{"cell_type":"markdown","source":["Set token index (i.e. 0 here) as the default index. This index is returned when the token is not found. If not set, it throws RuntimeError when the queried token is not found in the Vocabulary."],"metadata":{"id":"gJ4WN8TeIxWM"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"rSj92VRsHMw4","executionInfo":{"status":"ok","timestamp":1674916661665,"user_tz":-345,"elapsed":31,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"outputs":[],"source":["for lang in [SRC_LANG, TGT_LANG]:\n","  vocab[lang].set_default_index(specials['<UNK>'])"]},{"cell_type":"markdown","source":["## Encoder\n","\n","The encoder architecture is same as that used in seq2seq except the following two facts:\n","- We will be using single layer of RNN\n","- We will be using bidirectional RNN (forward + backward)\n","\n","As done in seq2seq, we initialize both forward and backward hidden states to a tensor of zeros. We get two context vectors one from each of forward and backward RNNs. However the decoder being unidirectional needs single context vector as input. To facilitate this we'll be concatinating two context vectors together."],"metadata":{"id":"0zxzPTDtI7_Q"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n","        super().__init__()\n","        self.embed = nn.Embedding(input_dim, emb_dim)\n","        self.rnn = nn.LSTM(emb_dim, enc_hid_dim, bidirectional=True)\n","        self.fc = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src):\n","        embedding = self.dropout(self.embed(src))  # [len(src), batch_size, emb_dim]\n","        output, (hidden, cell) = self.rnn(embedding)\n","        # output = [len(src), batch_size, hid_dim * n_directions]\n","        # hidden = cell = [n layers * n directions, batch size, hid dim]\n","        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))  # [batch_size, dec_hid_dim]\n","        return output, hidden"],"metadata":{"id":"GMPI4J-NI6Ja","executionInfo":{"status":"ok","timestamp":1674916661666,"user_tz":-345,"elapsed":31,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Attention\n","\n","The attention layer is important to decide which word in the encoder should we pay most attention to in order to predict the next word. The attention layer takes all the hidden states of encoders and only the previous hidden state of the decoder. The result is an attention vector whose elements have values between 0 and 1 and the softmax layer confirms these values sum to 1.\n","\n","We calculate the energy between encoder hidden states and previous decoder hidden state. However the shape of encoder and decoder hidden states do not match, because we have n encoder hidden states and a single decoder hidden state. So we repeat decoder hidden state n times and concatinate two tensors together. This `energy` determines how well each encoder hidden state matches the previous decoder hidden state.\n","\n","We currently have a [dec_hid_dim, src_len] tensor for each example in the batch. We want this to be [src_len] for each example in the batch as the attention should be over the length of the source sentence. This is achieved by multiplying the energy by a [1, dec_hid_dim] tensor `v`. We can think of `v` as the weights for a weighted sum of the energy across all encoder hidden states. These weights tell us how much we should attend to each token in the source sequence. The parameters of `v` are initialized randomly, but learned with the rest of the model via backpropagation. Note how `v` is not dependent on time, and the same `v` is used for each time-step of the decoding. We implement `v` as a linear layer without a bias."],"metadata":{"id":"aqanje52epeI"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"gKBW3voeHMw5","executionInfo":{"status":"ok","timestamp":1674916661667,"user_tz":-345,"elapsed":31,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"outputs":[],"source":["class BahdanauAttention(nn.Module):\n","    def __init__(self, enc_hid_dim, dec_hid_dim):\n","      super().__init__()\n","      self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n","      self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n","        \n","    def forward(self, hidden, encoder_outputs):\n","      src_len = encoder_outputs.shape[0]\n","      batch_size = encoder_outputs.shape[1]\n","      hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # Repeat decoder hidden state src_len times\n","      encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","      # hidden = [batch size, src len, dec hid dim]\n","      # encoder_outputs = [batch size, src len, enc hid dim * 2]\n","      energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))  # [batch size, src len, dec hid dim]\n","      attention = self.v(energy).squeeze(2)  # [batch size, src len]\n","      return F.softmax(attention, dim=1)"]},{"cell_type":"markdown","source":["## Decoder\n","\n","Decoder uses the attention vector to create a weighted source vector where attention vector `a` is the weight to encoder hidden states denoted by `encoder_outputs`."],"metadata":{"id":"QODTkb4OfhJX"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"FYAQOQu6HMw5","executionInfo":{"status":"ok","timestamp":1674916661668,"user_tz":-345,"elapsed":32,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n","        super().__init__()\n","        self.output_dim = output_dim\n","        self.attention = attention\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n","        self.fc = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, input, hidden, encoder_outputs): \n","        #input = [batch size]\n","        #hidden = [batch size, dec hid dim]\n","        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n","        input = input.unsqueeze(0)  # [1, batch size]\n","        embedded = self.dropout(self.embedding(input))  # [1, batch size, emb dim]\n","        a = self.attention(hidden, encoder_outputs)  # [batch size, src len]\n","        a = a.unsqueeze(1)  # [batch size, 1, src len]\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch size, src len, enc hid dim * 2]\n","        weighted = torch.bmm(a, encoder_outputs)  # [batch size, 1, enc hid dim * 2]\n","        weighted = weighted.permute(1, 0, 2)  # [1, batch size, enc hid dim * 2]\n","        rnn_input = torch.cat((embedded, weighted), dim = 2)  # [1, batch size, (enc hid dim * 2) + emb dim]\n","        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n","        #output = [seq len, batch size, dec hid dim * n directions]\n","        #hidden = [n layers * n directions, batch size, dec hid dim]\n","        assert (output == hidden).all()\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted = weighted.squeeze(0)\n","        prediction = self.fc(torch.cat((output, weighted, embedded), dim = 1))  # [batch size, output dim]\n","        return prediction, hidden.squeeze(0)"]},{"cell_type":"markdown","source":["## Seq2Seq\n","\n","Here decoder and encoder have different hidden dimensions."],"metadata":{"id":"EJ0b9FAdgeXx"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"ou1ZST66HMw5","executionInfo":{"status":"ok","timestamp":1674916661669,"user_tz":-345,"elapsed":31,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        \n","    def forward(self, src, tgt, teacher_forcing_ratio = 0.5):\n","        #src = [src len, batch size]\n","        #tgt = [tgt len, batch size]\n","        batch_size = src.shape[1]\n","        tgt_len = tgt.shape[0]\n","        tgt_vocab_size = self.decoder.output_dim\n","        outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size).to(self.device)  # tensor to store decoder outputs\n","        encoder_outputs, hidden = self.encoder(src)\n","        input = tgt[0, :]  # First input to decoder is the <sos> tokens\n","        for t in range(1, tgt_len):\n","            output, hidden = self.decoder(input, hidden, encoder_outputs)\n","            outputs[t] = output  # Store predictions in a tensor initialized above\n","            teacher_force = random.random() < teacher_forcing_ratio  # Decide if we are going to use teacher forcing\n","            top1 = output.argmax(1)  # Get the highest predicted token\n","            input = tgt[t] if teacher_force else top1\n","        return outputs"]},{"cell_type":"markdown","source":["## Training Seq2Seq Model"],"metadata":{"id":"B33BcH0FrqrX"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"ZGGSNCO2HMw6","executionInfo":{"status":"ok","timestamp":1674916661670,"user_tz":-345,"elapsed":31,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"outputs":[],"source":["INPUT_DIM = len(vocab[SRC_LANG])\n","OUTPUT_DIM = len(vocab[TGT_LANG])\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","ENC_HID_DIM = 512\n","DEC_HID_DIM = 512\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","BATCH_SIZE = 128\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","source":["attn = BahdanauAttention(ENC_HID_DIM, DEC_HID_DIM)\n","encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","\n","model = Seq2Seq(encoder, decoder, device).to(device)"],"metadata":{"id":"X_313S-usAbt","executionInfo":{"status":"ok","timestamp":1674916663342,"user_tz":-345,"elapsed":1703,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","            \n","model.apply(init_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWRpbFYMsAdy","executionInfo":{"status":"ok","timestamp":1674916663345,"user_tz":-345,"elapsed":57,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}},"outputId":"1c565b27-0002-4bc5-a52c-06056be1c04b"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embed): Embedding(19214, 256)\n","    (rnn): LSTM(256, 512, bidirectional=True)\n","    (fc): Linear(in_features=1024, out_features=512, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): BahdanauAttention(\n","      (attn): Linear(in_features=1536, out_features=512, bias=True)\n","      (v): Linear(in_features=512, out_features=1, bias=False)\n","    )\n","    (embedding): Embedding(10837, 256)\n","    (rnn): GRU(1280, 512)\n","    (fc): Linear(in_features=1792, out_features=10837, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NfG3eL0sOxa","executionInfo":{"status":"ok","timestamp":1674916663346,"user_tz":-345,"elapsed":49,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}},"outputId":"cf4d1877-19d2-4e80-acea-0be700462049"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 34,345,557 trainable parameters\n"]}]},{"cell_type":"code","source":["optimizer = Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index=specials['<PAD>'])"],"metadata":{"id":"zuRvVWe4sAf7","executionInfo":{"status":"ok","timestamp":1674916663348,"user_tz":-345,"elapsed":41,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","\n","def tensor_transform(token_id: List[int]):\n","    return torch.cat((torch.tensor([specials['<SOS>']]), torch.tensor(token_id), torch.tensor([specials['<EOS>']])))"],"metadata":{"id":"2oT4Gq7JsAis","executionInfo":{"status":"ok","timestamp":1674916663350,"user_tz":-345,"elapsed":41,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["text_transform = {}\n","for ln in [SRC_LANG, TGT_LANG]:\n","    text_transform[ln] = sequential_transforms(tokenizer[ln], vocab[ln], tensor_transform)"],"metadata":{"id":"yjBqmxMmsAlA","executionInfo":{"status":"ok","timestamp":1674916663351,"user_tz":-345,"elapsed":41,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[SRC_LANG](src_sample))\n","        tgt_batch.append(text_transform[TGT_LANG](tgt_sample))\n","\n","    src_batch = pad_sequence(src_batch, padding_value=specials['<PAD>'])\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=specials['<PAD>'])\n","    return src_batch, tgt_batch"],"metadata":{"id":"JZgkxySUsAnS","executionInfo":{"status":"ok","timestamp":1674916663352,"user_tz":-345,"elapsed":40,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["train_dataloader = DataLoader(train_iterator, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n","valid_dataloader = DataLoader(valid_iterator, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n","test_dataloader = DataLoader(test_iterator, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)"],"metadata":{"id":"h_8q2pbFsAqF","executionInfo":{"status":"ok","timestamp":1674916663354,"user_tz":-345,"elapsed":41,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def train(model, dataloader, optimizer, criterion, clip):\n","    model.train()\n","    epoch_loss = 0\n","    batch_idx = 0\n","    for src, tgt in dataloader:\n","        src = src.to(device)  # [len(src), batch_size]\n","        tgt = tgt.to(device)  # [len(tgt), batch_size]\n","        optimizer.zero_grad()\n","        output = model(src, tgt)  # [len(tgt), batch_size, output_dim]\n","        output_dim = output.shape[-1]\n","        output = output[1:].view(-1, output_dim)\n","        tgt = tgt[1:].view(-1)\n","        loss = criterion(output, tgt)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        batch_idx += 1\n","    return epoch_loss / batch_idx"],"metadata":{"id":"wzcZddljvoDU","executionInfo":{"status":"ok","timestamp":1674916663355,"user_tz":-345,"elapsed":40,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, dataloader, criterion):\n","    model.eval()\n","    epoch_loss = 0\n","    batch_idx = 0\n","    with torch.no_grad():\n","        for src, tgt in dataloader:\n","            src = src.to(device)  # [len(src), batch_size]\n","            tgt = tgt.to(device)  # [len(tgt), batch_size]\n","            output = model(src, tgt, 0)  # Teacher forcing is turned off\n","            output_dim = output.shape[-1]\n","            output = output[1:].view(-1, output_dim)  # [(len(tgt) - 1) * batch size, output_dim]\n","            tgt = tgt[1:].view(-1)  # Shape = [(len(tgt) - 1) * batch size]\n","            loss = criterion(output, tgt)\n","            epoch_loss += loss.item()\n","            batch_idx += 1\n","    return epoch_loss / batch_idx"],"metadata":{"id":"f1ytkyQ5sqXE","executionInfo":{"status":"ok","timestamp":1674916663357,"user_tz":-345,"elapsed":40,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"metadata":{"id":"BG9ywRc8sqZ5","executionInfo":{"status":"ok","timestamp":1674916663359,"user_tz":-345,"elapsed":41,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 10\n","CLIP = 1\n","\n","if not os.path.exists('./../models'):\n","  os.mkdir('./../models')"],"metadata":{"id":"7dMIiJ8Xsqc7","executionInfo":{"status":"ok","timestamp":1674916663360,"user_tz":-345,"elapsed":40,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["best_valid_loss = float('inf')\n","for epoch in range(EPOCHS):\n","    start_time = time.time()\n","    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_dataloader, criterion)\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), './../models/bahdanau.pt')\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}:{epoch_secs} | Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OL-1Cp73sqf8","executionInfo":{"status":"ok","timestamp":1674917787419,"user_tz":-345,"elapsed":958908,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}},"outputId":"096bccf9-67d9-4133-b6c2-00e2af439301"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Time: 1:49 | Train Loss: 5.177 | Val Loss: 5.005\n","Epoch: 02 | Time: 1:50 | Train Loss: 4.284 | Val Loss: 4.612\n","Epoch: 03 | Time: 1:51 | Train Loss: 3.766 | Val Loss: 4.290\n","Epoch: 04 | Time: 1:52 | Train Loss: 3.262 | Val Loss: 3.883\n","Epoch: 05 | Time: 1:53 | Train Loss: 2.804 | Val Loss: 3.730\n","Epoch: 06 | Time: 1:53 | Train Loss: 2.417 | Val Loss: 3.646\n","Epoch: 07 | Time: 1:52 | Train Loss: 2.087 | Val Loss: 3.553\n","Epoch: 08 | Time: 1:53 | Train Loss: 1.817 | Val Loss: 3.629\n","Epoch: 09 | Time: 1:52 | Train Loss: 1.640 | Val Loss: 3.623\n","Epoch: 10 | Time: 1:53 | Train Loss: 1.485 | Val Loss: 3.685\n"]}]},{"cell_type":"code","source":["# model.load_state_dict(torch.load('./../models/bahdanau.pt'))\n","# test_loss = evaluate(model, test_dataloader, criterion)\n","\n","# print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"],"metadata":{"id":"0WBRDjP_sqi4","executionInfo":{"status":"ok","timestamp":1674917787420,"user_tz":-345,"elapsed":2,"user":{"displayName":"Arun Ghimire","userId":"12379322823635086573"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1J0mD_lwHMw6"},"source":["## References\n","\n","- [The Power of Attention in Deep Learning](https://www.youtube.com/watch?v=Qu81irGlR-0)\n","- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n","- [The Bahdanau Attention Mechanism](https://machinelearningmastery.com/the-bahdanau-attention-mechanism/#:~:text=The%20Bahdanau%20attention%20was%20proposed,mechanism%20for%20neural%20machine%20translation.)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}