{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bahdanau Attention (Additive Attention)\n",
    "\n",
    "One of the motivations behind Bahdanau Attention approach was the use of a fixed-length context vector in the basic encoderâ€“decoder approach. This limitation makes the basic encoder-decoder approach to underperform with long sentences. In basic encoder-decoder approach, the last element of a sequence contains the memory of all the previous elements and thus form a fixed-dimension context vector. But in case of Bahdanau attention approach:\n",
    "\n",
    "- First, we initialize the Decoder states by using the last states of the Encoder as usual\n",
    "- Then at each decoding time step:\n",
    "    - We use Encoder's all hidden states and the previous Decoder's output to calculate a Context Vector by applying an Attention Mechanism\n",
    "    - Lastly, we concatenate the Context Vector with the previous Decoder's output to create the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_speaker, n_tags, n_embed_text, n_embed_speaker, n_embed_tags, n_hidden_enc, n_layers, n_hidden_dec, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden_enc = n_hidden_enc\n",
    "        self.text_embedding = nn.Embedding(n_vocab, n_embed_text)\n",
    "        self.tag_embedding = nn.Embedding(n_tags, n_embed_tags)\n",
    "        self.speaker_embedding = nn.Embedding(n_speaker, n_embed_speaker)\n",
    "        \n",
    "        self.bdGRU = nn.GRU(n_embed_text + n_embed_speaker + n_embed_tags, \n",
    "                            n_hidden_enc, \n",
    "                            n_layers,\n",
    "                            bidirectional=True, batch_first=True,\n",
    "                            dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden_enc, n_hidden_dec)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, s, t, h):\n",
    "        ''' x, s, t: [b, seq_len] '''\n",
    "        \n",
    "        embedded = torch.cat((self.text_embedding(x), self.speaker_embedding(s)), dim=2)  \n",
    "        embedded = torch.cat((embedded, self.tag_embedding(t)), dim=2) #[b, seq_len, n_embed_text + _tags + _speaker]\n",
    "        \n",
    "        last_layer_enc, last_h_enc = self.bdGRU(embedded, h)\n",
    "        \n",
    "        last_layer_enc = self.dropout(last_layer_enc) #[b, seq_len, 2*n_hidden_enc]\n",
    "        last_h_enc = self.dropout(last_h_enc.permute(1, 0, 2))         #[b, 2*n_layer, n_hidden_enc]\n",
    "\n",
    "        # this is to be decoder's 1st hidden state\n",
    "        for i in range(0, -self.n_layers, -2):\n",
    "            if i==0:\n",
    "                last_h_enc_sum = last_h_enc[:, i-1, :] + last_h_enc[:, i-2, :] #[b, n_hidden_enc]\n",
    "                last_h_enc_sum = last_h_enc_sum.unsqueeze(1)\n",
    "            else:\n",
    "                last_h_enc_sum = torch.cat((last_h_enc_sum, \n",
    "                                            (last_h_enc[:, i-1, :] + last_h_enc[:, i-2, :]).unsqueeze(1)), dim=1)\n",
    "        \n",
    "\n",
    "        last_h_enc_sum = self.fc(last_h_enc_sum) #[b, n_layers, n_hidden_dec]\n",
    "\n",
    "        return last_layer_enc, last_h_enc_sum   #[b, seq_len, 2*n_hidden_enc], [b, n_layer, n_hidden_enc]\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weight = next(self.parameters()).data\n",
    "    \n",
    "        h = weight.new(2*self.n_layers, batch_size, self.n_hidden_enc).zero_().to(device)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden_enc, n_hidden_dec):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.h_hidden_enc = n_hidden_enc\n",
    "        self.h_hidden_dec = n_hidden_dec\n",
    "        \n",
    "        self.W = nn.Linear(2*n_hidden_enc + n_hidden_dec, n_hidden_dec, bias=False) \n",
    "        self.V = nn.Parameter(torch.rand(n_hidden_dec))\n",
    "        \n",
    "    \n",
    "    def forward(self, hidden_dec, last_layer_enc):\n",
    "        ''' \n",
    "            PARAMS:           \n",
    "                hidden_dec:     [b, n_layers, n_hidden_dec]    (1st hidden_dec = encoder's last_h's last layer)                 \n",
    "                last_layer_enc: [b, seq_len, n_hidden_enc * 2] \n",
    "            \n",
    "            RETURN:\n",
    "                att_weights:    [b, src_seq_len] \n",
    "        '''\n",
    "        \n",
    "        batch_size = last_layer_enc.size(0)\n",
    "        src_seq_len = last_layer_enc.size(1)\n",
    "\n",
    "        hidden_dec = hidden_dec[:, -1, :].unsqueeze(1).repeat(1, src_seq_len, 1)         #[b, src_seq_len, n_hidden_dec]\n",
    "\n",
    "        tanh_W_s_h = torch.tanh(self.W(torch.cat((hidden_dec, last_layer_enc), dim=2)))  #[b, src_seq_len, n_hidden_dec]\n",
    "        tanh_W_s_h = tanh_W_s_h.permute(0, 2, 1)       #[b, n_hidde_dec, seq_len]\n",
    "        \n",
    "        V = self.V.repeat(batch_size, 1).unsqueeze(1)  #[b, 1, n_hidden_dec]\n",
    "        e = torch.bmm(V, tanh_W_s_h).squeeze(1)        #[b, seq_len]\n",
    "        \n",
    "        att_weights = F.softmax(e, dim=1)              #[b, src_seq_len]\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_output, n_embed, n_hidden_enc, n_hidden_dec, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_output = n_output\n",
    "        self.embedding = nn.Embedding(n_output, n_embed)\n",
    "        \n",
    "        self.GRU = nn.GRU(n_embed + n_hidden_enc*2, n_hidden_dec, n_layers,\n",
    "                          bidirectional=False, batch_first=True,\n",
    "                          dropout=dropout) \n",
    "        \n",
    "        self.attention = Attention(n_hidden_enc, n_hidden_dec)\n",
    "        \n",
    "        self.fc_final = nn.Linear(n_embed + n_hidden_enc*2 + n_hidden_dec, n_output)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, target, hidden_dec, last_layer_enc):\n",
    "        ''' \n",
    "            target:         [b] \n",
    "            hidden_dec:     [b, n_layers, n_hidden_dec]      (1st hidden_dec = encoder's last_h's last layer)\n",
    "            last_layer_enc: [b, seq_len, n_hidden_enc * 2]   (* 2 since bi-directional)  \n",
    "            --> not to be confused with encoder's last hidden state. last_layer_enc is ALL hidden states (of timesteps 0,1,...,t-1) of the last LAYER.\n",
    "        '''\n",
    "        \n",
    "        ######################## 1. TARGET EMBEDDINGS ######################### \n",
    "        target = target.unsqueeze(1) #[b, 1] : since n_output = 1\n",
    "        embedded_trg = self.embedding(target)  #[b, 1, n_embed]\n",
    "\n",
    "\n",
    "        ################## 2. CALCULATE ATTENTION WEIGHTS #####################      \n",
    "        att_weights = self.attention(hidden_dec, last_layer_enc)  #[b, input_seq_len]\n",
    "        att_weights = att_weights.unsqueeze(1)   #[b, 1, input_seq_len]\n",
    "\n",
    "\n",
    "        ###################### 3. CALCULATE WEIGHTED SUM ######################\n",
    "        weighted_sum = torch.bmm(att_weights, last_layer_enc) #[b, 1, n_hidden_enc*2]\n",
    "        \n",
    "        \n",
    "        ############################# 4. GRU LAYER ############################\n",
    "        gru_input = torch.cat((embedded_trg, weighted_sum), dim=2) #[b, 1, n_embed + n_hidden_enc*2]\n",
    "\n",
    "        last_layer_dec, last_h_dec = self.GRU(gru_input, hidden_dec.permute(1, 0, 2))\n",
    "        # last_layer_dec: [b, trg_seq_len, n_hidden_dec]\n",
    "        last_h_dec = last_h_dec.permute(1, 0, 2)  #[b, n_layers, n_hidden_dec]\n",
    "        \n",
    "        \n",
    "        ########################### 5. FINAL FC LAYER #########################\n",
    "        fc_in = torch.cat((embedded_trg.squeeze(1),           #[b, n_embed]\n",
    "                           weighted_sum.squeeze(1),           #[b, n_hidden_enc*2]\n",
    "                           last_layer_dec.squeeze(1)), dim=1) #[b, n_hidden_dec]                           \n",
    "                           \n",
    "       \n",
    "        output = self.fc_final(fc_in) #[b, n_output]\n",
    "        \n",
    "        return output, last_h_dec, att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_speaker, n_tags, \n",
    "                       n_embed_text, n_embed_speaker, n_embed_tags, n_embed_dec, \n",
    "                       n_hidden_enc, n_hidden_dec, n_layers, \n",
    "                       n_output, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(n_vocab=n_vocab, n_speaker=n_speaker, n_tags=n_tags,\n",
    "                               n_embed_text=n_embed_text, n_embed_speaker=n_embed_speaker, n_embed_tags=n_embed_tags,\n",
    "                               n_hidden_enc=n_hidden_enc, n_layers=n_layers, n_hidden_dec=n_hidden_dec, \n",
    "                               dropout=dropout)\n",
    "        \n",
    "        self.decoder = Decoder(n_output=n_output, \n",
    "                               n_embed=n_embed_dec, \n",
    "                               n_hidden_enc=n_hidden_enc, n_hidden_dec=n_hidden_dec, n_layers=n_layers, \n",
    "                               dropout=dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, speakers, tags, targets, tf_ratio=0.5):\n",
    "        ''' inputs:  [b, input_seq_len(200)]\n",
    "            targets: [b, input_seq_len(200)]'''\n",
    "            \n",
    "        ###########################  1. ENCODER  ##############################\n",
    "        h = self.encoder.init_hidden(inputs.size(0))\n",
    "        \n",
    "        last_layer_enc, last_h_enc = self.encoder(inputs, speakers, tags, h)              \n",
    "        \n",
    "            \n",
    "        ###########################  2. DECODER  ##############################\n",
    "        hidden_dec = last_h_enc       #[b, n_layers, n_hidden_dec]\n",
    "        \n",
    "        trg_seq_len = targets.size(1)\n",
    "        \n",
    "        b = inputs.size(0)\n",
    "        n_output = self.decoder.n_output\n",
    "        output = targets[:, 0]\n",
    "        \n",
    "        outputs = torch.zeros(b, n_output, trg_seq_len).cuda()\n",
    "        \n",
    "        for t in range(1, trg_seq_len, 1):\n",
    "            output, hidden_dec = self.decoder(output, hidden_dec, last_layer_enc)\n",
    "            outputs[:, :, t] = output #output: [b, n_output]\n",
    "\n",
    "            if random.random() < tf_ratio:\n",
    "                output = targets[:, t]\n",
    "                \n",
    "            else:\n",
    "                output = output.max(dim=1)[1]\n",
    "        \n",
    "        return outputs  #[b, n_output, trg_seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [The Power of Attention in Deep Learning](https://www.youtube.com/watch?v=Qu81irGlR-0)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "- [The Bahdanau Attention Mechanism](https://machinelearningmastery.com/the-bahdanau-attention-mechanism/#:~:text=The%20Bahdanau%20attention%20was%20proposed,mechanism%20for%20neural%20machine%20translation.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
