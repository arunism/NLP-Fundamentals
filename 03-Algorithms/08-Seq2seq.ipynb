{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence\n",
    "\n",
    "Sequence to Sequence are normal rnns despite the fact that they consists of two different rnn structures working together. The first one is called the `Encoder` and the another one is called the `Decoder`. Encoder encodes the input and generates a final vector specifically known as the `Context Vector`. The decoder then takes this context vector as an input and decodes it to generate the required result. This has a number of applications in the world of NLP and Machine Learning like Machine Translation, Speech recognition, Image Captioning and many more.\n",
    "\n",
    "For this task, we will use `Multi30k` dataset from torchtext library that yields a pair of source-target raw sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from typing import Iterable, List\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator as bvfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANG = 'de'\n",
    "TGT_LANG = 'en'\n",
    "specials = {'<UNK>': 0, '<PAD>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "\n",
    "tokenizer = dict()\n",
    "vocab = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "\n",
    "```\n",
    "pip install -U torchdata\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer[SRC_LANG] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "tokenizer[TGT_LANG] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANG: 0, TGT_LANG: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer[language](data_sample[language_index[language]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in [SRC_LANG, TGT_LANG]:\n",
    "    train_iterator, valid_iterator, test_iterator = Multi30k()    # Training data Iterator\n",
    "    vocab[lang] = bvfi(yield_tokens(train_iterator, lang), min_freq=1, specials=specials.keys(), special_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set <UNK> token index (i.e. 0 here) as the default index. This index is returned when the token is not found. If not set, it throws RuntimeError when the queried token is not found in the Vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in [SRC_LANG, TGT_LANG]:\n",
    "  vocab[lang].set_default_index(specials['<UNK>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Seq2seq Model\n",
    "\n",
    "We inherit each of the three modules below from `torch.nn.Module` and use the super().__init__() as some boilerplate code. The encoder takes the following arguments:\n",
    "\n",
    "- `input_dim:` Dimension/Size of the one-hot vectors that will be input to the encoder. This is equal to source vocabulary size.\n",
    "- `emb_dim:` Dimension of the embedding layer. This layer converts the one-hot vectors into dense vectors with emb_dim dimensions.\n",
    "- `hid_dim:` Dimension of hidden and cell states.\n",
    "- `n_layers:` Number of layers in RNN.\n",
    "- `dropout:` Amount of dropout. This is a regularization parameter to prevent overfitting.\n",
    "\n",
    "In this case, `n_directions` will always be 1 i.e. we are cosidering unidirectional RNNs in this tutorial. However note that bidirectional RNNs will have n_directions as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedding = self.dropout(self.embed(src))  # [len(src), batch_size]\n",
    "        output, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        # hidden = cell = [n layers * n directions, batch size, hid dim]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments and initialization are similar to the Encoder, except we now have an `output_dim` which is the size of the vocabulary for the output/target. There is also the addition of the `Linear` layer, used to make the predictions from the top layer hidden state.\n",
    "\n",
    "Within the `forward` method, we accept a batch of input tokens, previous hidden states and previous cell states. As we are only decoding one token at a time, the input tokens will always have a sequence length of 1. We `unsqueeze` the input tokens to add a sentence length dimension of 1.\n",
    "\n",
    "**`Note:`** as we always have a sequence length of 1, we could use `nn.LSTMCell`, instead of `nn.LSTM`, as it is designed to handle a batch of inputs that aren't necessarily in a sequence. LSTMCell is just a single cell and LSTM is a wrapper around potentially multiple cells. Using the LSTMCell in this case would mean we don't have to `unsqueeze` to add a fake sequence length dimension, but we would need one LSTMCell per layer in the decoder and to ensure each LSTMCell receives the correct initial hidden state from the encoder. All of this makes the code less concise - hence the decision to stick with the regular LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.embed = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs, hidden, cell):\n",
    "        inputs = inputs.unsqueeze(0)  # [1, batch_size]\n",
    "        embedding = self.dropout(self.embed(inputs))  # [1, batch_size, emb_dim]\n",
    "        output, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # output = [seq len, batch size, hid dim * n directions]\n",
    "        # hidden = cell = [n layers * n directions, batch size, hid dim]\n",
    "        # seq len and n directions will always be 1 in the decoder\n",
    "        prediction = self.fc(output.squeeze(0))  # [batch size, output dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq model takes in an Encoder, Decoder, and a device (used to place tensors on the GPU, if it exists).\n",
    "\n",
    "For this implementation, we have to ensure that the number of layers and the hidden (and cell) dimensions are equal in the Encoder and Decoder. This is not always the case, we do not necessarily need the same number of layers or the same hidden dimension sizes in a sequence-to-sequence model. However, if we did something like having a different number of layers then we would need to make decisions about how this is handled. For example, if our encoder has 2 layers and our decoder only has 1, how is this handled? Do we average the two context vectors output by the decoder? Do we pass both through a linear layer? Do we only use the context vector from the highest layer? etc.\n",
    "\n",
    "Our forward method takes the source sentence, target sentence and a teacher forcing ratio. `teacher_forcing` is used when training our model. When decoding, at each time-step we will predict what the next token in the target sequence will be from the previous tokens decoded. With probability equal to the teaching forcing ratio we will use the actual ground-truth next token in the sequence as the input to the decoder during the next time-step. However, with probability `1 - teacher_forcing`, we will use the token that the model predicted as the next input to the model, even if it doesn't match the actual next token in the sequence.\n",
    "\n",
    "The first thing we do in the forward method is to create an outputs tensor that will store all of our predictions. We then feed the input/source sentence, src, into the encoder and receive out final hidden and cell states.\n",
    "\n",
    "The first input to the decoder is the start of sequence (<SOS>) token. As our *tgt* tensor already has the <SOS> token appended (all the way back when we defined the init_token in our *TGT* field) we get our \n",
    " by slicing into it. We know how long our target sentences should be (max_len), so we loop that many times. The last token input into the decoder is the one before the <EOS> token - the <EOS> token is never input into the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \"Encoder and decoder must have equal number of layers!\"\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing=0.5):\n",
    "        batch_size = tgt.shape[1]\n",
    "        tgt_len = tgt.shape[0]\n",
    "        tgt_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        inputs = tgt[0, :]  # First input to the decoder is the <sos> tokens\n",
    "        \n",
    "        for target in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(inputs, hidden, cell)\n",
    "            outputs[target] = output  # Place predictions in a tensor holding predictions for each token\n",
    "            teacher_force = random.random() < teacher_forcing  # Decide if we are going to use teacher forcing\n",
    "            top = output.argmax(1)  # Select the highest predicted token from our predictions\n",
    "            # If teacher forching use the actual next token else use the predicted one\n",
    "            inputs = tgt[target] if teacher_force else top \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Seq2seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab[SRC_LANG])\n",
    "OUTPUT_DIM = len(vocab[TGT_LANG])\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize the weights of our model. In the paper they state they initialize all weights from a uniform distribution between -0.08 and +0.08. When using apply, the init_weights function will be called on every module and sub-module within our model. For each module we loop through all of the parameters and sample them from a uniform distribution with `nn.init.uniform_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedding(19214, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): Embedding(10837, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc): Linear(in_features=512, out_features=10837, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 20,608,853 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our optimizer and loss function. The `CrossEntropyLoss` function calculates both the log softmax as well as the negative log-likelihood of our predictions. Our loss function calculates the average loss per token, however by passing the index of the <PAD> token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=specials['<PAD>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.train()` sets our model to training mode. This will turn on dropout (and batch normalization, which we aren't using) and then iterate through our data. As stated before, our decoder loop starts at 1, not 0. This means the 0th element of our outputs tensor remains all zeros. When we calculate the loss, we cut off the first element of each tensor. `loss.backward()` is used to calculate gradients. We will also clip the gradients to prevent them from exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_iterator, batch_size=BATCH_SIZE)\n",
    "valid_dataloader = DataLoader(valid_iterator, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    batch = 0\n",
    "    for src, tgt in dataloader:\n",
    "#         src = src.to(device)  # [len(src), batch_size]\n",
    "#         tgt = tgt.to(device)  # [len(tgt), batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)  # [len(tgt), batch_size, output_dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        batch += 1\n",
    "    return epoch_loss / batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn the evaluation mode on, we'll use `model.eval()`. This will turn dropout (and batch normalization) off. We use the `torch.no_grad()` block to ensure no gradients are calculated within the block. This reduces memory consumption and speeds things up. The iteration loop is similar to the training loop except the fact that we are not updaing parameters and teacher forcing is also turned off. This will cause the model to only use it's own predictions to make further predictions within a sentence, which mirrors how it would be used in deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    batch = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "#             src = src.to(device)  # [len(src), batch_size]\n",
    "#             tgt = tgt.to(device)  # [len(tgt), batch_size]\n",
    "            output = model(src, tgt, 0)  # Teacher forcing is turned off\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)  # [(len(tgt) - 1) * batch size, output_dim]\n",
    "            tgt = tgt[1:].view(-1)  # Shape = [(len(tgt) - 1) * batch size]\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "            batch += 1\n",
    "    return epoch_loss / batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to track the time taken by each epoch during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, its training time. At each epoch, we'll be checking if our model has achieved the best validation loss so far. If it has, we'll update our best validation loss and save the parameters of our model (called `state_dict` in PyTorch). Then, when we come to test our model, we'll use the saved parameters used to achieve the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      3\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_dataloader, criterion)\n\u001b[1;32m      6\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m src, tgt \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#         src = src.to(device)  # [len(src), batch_size]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#         tgt = tgt.to(device)  # [len(tgt), batch_size]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [len(tgt), batch_size, output_dim]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m         output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_dim)\n",
      "File \u001b[0;32m~/Documents/NLP/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, tgt, teacher_forcing)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt, teacher_forcing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m     tgt_len \u001b[38;5;241m=\u001b[39m tgt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m     tgt_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39moutput_dim\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './../models/seq2seq.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}:{epoch_secs} | Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./../models/seq2seq.pt'))\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "- [Language Translation with nn, transformer and torchtext](https://pytorch.org/tutorials/beginner/translation_transformer.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a9e37ffee3a5a10c5d687ae20c51ee3d4ba310e8eddff89fa7a028272d595a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
