{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence\n",
    "\n",
    "Sequence to Sequence are normal rnns despite the fact that they consists of two different rnn structures working together. The first one is called the `Encoder` and the another one is called the `Decoder`. Encoder encodes the input and generates a final vector specifically known as the `Context Vector`. The decoder then takes this context vector as an input and decodes it to generate the required result. This has a number of applications in the world of NLP and Machine Learning like Machine Translation, Speech recognition, Image Captioning and many more.\n",
    "\n",
    "For this task, we will use `Multi30k` dataset from torchtext library that yields a pair of source-target raw sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from typing import Iterable, List\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANG = 'de'\n",
    "TGT_LANG = 'en'\n",
    "specials = {'<UNK>': 0, '<PAD>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "\n",
    "tokens = dict()\n",
    "vocab = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "\n",
    "```\n",
    "pip install -U torchdata\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[SRC_LANG] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "tokens[TGT_LANG] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANG: 0, TGT_LANG: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield tokens[language](data_sample[language_index[language]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in [SRC_LANG, TGT_LANG]:\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANG, TGT_LANG))    # Training data Iterator\n",
    "    vocab[lang] = build_vocab_from_iterator(yield_tokens(train_iter, lang), min_freq=1, specials=specials.keys(), special_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set <UNK> token index (i.e. 0 here) as the default index. This index is returned when the token is not found. If not set, it throws RuntimeError when the queried token is not found in the Vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in [SRC_LANG, TGT_LANG]:\n",
    "  vocab[lang].set_default_index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Seq2seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Seq2seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "- [LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT](https://pytorch.org/tutorials/beginner/translation_transformer.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a9e37ffee3a5a10c5d687ae20c51ee3d4ba310e8eddff89fa7a028272d595a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
