{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU)\n",
    "\n",
    "Similar to LSTM, GRU is used to combat the problem of vanishing gradient.\n",
    "\n",
    "- The GRU has two gates, LSTM has three gates.\n",
    "- GRU does not possess any internal memory, they donâ€™t have an output gate that is present in LSTM.\n",
    "- In LSTM the input gate and target gate are coupled by an update gate and in GRU reset gate is applied directly to the previous hidden state. In LSTM the responsibility of reset gate is taken by the two gates i.e., input and target. \n",
    "\n",
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch import LongTensor\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['Gated', 'Recurrent', 'Unit']   # Dummy data of batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', 'G', 'R', 'U', 'a', 'c', 'd', 'e', 'i', 'n', 'r', 't', 'u']\n",
      "Vocabulary Index: [[1, 4, 11, 7, 6], [2, 7, 5, 12, 10, 10, 7, 9, 11], [3, 9, 8, 11]]\n"
     ]
    }
   ],
   "source": [
    "vocab = [''] + sorted(set([char for seq in data for char in seq]))\n",
    "vocab2idx = [[vocab.index(token) for token in seq] for seq in data]\n",
    "print('Vocabulary:', vocab)\n",
    "print('Vocabulary Index:', vocab2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  4, 11,  7,  6,  0,  0,  0,  0],\n",
      "        [ 2,  7,  5, 12, 10, 10,  7,  9, 11],\n",
      "        [ 3,  9,  8, 11,  0,  0,  0,  0,  0]])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  7,  9],\n",
      "        [11,  5,  8],\n",
      "        [ 7, 12, 11],\n",
      "        [ 6, 10,  0],\n",
      "        [ 0, 10,  0],\n",
      "        [ 0,  7,  0],\n",
      "        [ 0,  9,  0],\n",
      "        [ 0, 11,  0]])\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = LongTensor([len(seq) for seq in vocab2idx])\n",
    "sequence_tensor = Variable(torch.zeros(len(vocab2idx), seq_lengths.max(), dtype=torch.long))\n",
    "\n",
    "for idx, (seq, seq_len) in enumerate(zip(vocab2idx, seq_lengths)):\n",
    "    sequence_tensor[idx, :seq_len] = LongTensor(seq)\n",
    "\n",
    "print(sequence_tensor)\n",
    "\n",
    "# convert the input into time major format (Transpose)\n",
    "sequence_tensor = sequence_tensor.t()    # Shape: (max_len, batch_size)\n",
    "print(sequence_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model Implementation\n",
    "\n",
    "Here we will define model classes inherited from pytorch's `nn.Module`. The `__init__()` method is used to instanciate each model classes while `forward()` method is used to define the model solution. The shape of different elements of `forward()` module are listed below:\n",
    "\n",
    "- *input* Shape: (max_len, batch_size)\n",
    "- *embed* Shape: (max_len, batch_size, embed_dim)\n",
    "- *output* Shape: (max_len, batch_size, hidden_size)\n",
    "- *hidden* Shape: (num_layers * num_directions, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "hidden_dim = 5\n",
    "embed_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, bidirectional):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embed = self.embedding(input)\n",
    "        output, hidden = self.gru(embed)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Single Layer Unidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([9, 3])\n",
      "Output shape: torch.Size([9, 3, 5])\n",
      "Hidden shape: torch.Size([1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "n_layers = 1\n",
    "bidirectional = False\n",
    "\n",
    "model = GRUModel(input_dim, embed_dim, hidden_dim, n_layers, bidirectional)\n",
    "output, hidden = model(sequence_tensor)\n",
    "\n",
    "print(f\"Input shape: {sequence_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final output must be same as Hidden state in case of Single layer uni-directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (output[-1, :, :] == hidden[0]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Single Layer Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([9, 3])\n",
      "Output shape: torch.Size([9, 3, 10])\n",
      "Hidden shape: torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "n_layers = 1\n",
    "bidirectional = True\n",
    "\n",
    "model = GRUModel(input_dim, embed_dim, hidden_dim, n_layers, bidirectional)\n",
    "output, hidden = model(sequence_tensor)\n",
    "\n",
    "print(f\"Input shape: {sequence_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First hidden_dim of output at last time step must be same as Final Forward Hidden state in case of Single layer bi-directional GRU\n",
    "- Last hidden_dim of output at initial time step must be same as Final Backward Hidden state in case of Single layer bi-directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (output[-1, :, :hidden_dim] == hidden[0]).all()\n",
    "assert (output[0, :, hidden_dim:] == hidden[-1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Multiple Layer Unidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([9, 3])\n",
      "Output shape: torch.Size([9, 3, 5])\n",
      "Hidden shape: torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "n_layers = 2\n",
    "bidirectional = False\n",
    "\n",
    "model = GRUModel(input_dim, embed_dim, hidden_dim, n_layers, bidirectional)\n",
    "output, hidden = model(sequence_tensor)\n",
    "\n",
    "print(f\"Input shape: {sequence_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final output must be same as Final Hidden state in case of Multi layer uni-directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (output[-1, :, :] == hidden[-1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Multiple Layer Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([9, 3])\n",
      "Output shape: torch.Size([9, 3, 10])\n",
      "Hidden shape: torch.Size([4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "n_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "model = GRUModel(input_dim, embed_dim, hidden_dim, n_layers, bidirectional)\n",
    "output, hidden = model(sequence_tensor)\n",
    "\n",
    "print(f\"Input shape: {sequence_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped hidden shape: torch.Size([2, 2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = sequence_tensor.shape[1]\n",
    "hidden = hidden.view(n_layers, 2, batch_size, hidden_dim)\n",
    "print(f\"Reshaped hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First hidden_dim of output at last time step must be same as Final Forward Hidden state of final layer in case of Multi layer bi-directional GRU\n",
    "- Last hidden_dim of output at initial time step must be same as Final Backward Hidden state of final layer in case of Multi layer bi-directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (output[-1, :, :hidden_dim] == hidden[-1][0]).all()\n",
    "assert (output[0, :, hidden_dim:] == hidden[-1][1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Understanding GRU Networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)\n",
    "- [Introduction to Gated Recurrent Unit (GRU)](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-gated-recurrent-unit-gru/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
