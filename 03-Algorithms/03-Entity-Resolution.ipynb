{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution\n",
    "\n",
    "**Entity** refers to any unique object (person, place, organization, etc.) with real and independent existence. **Entity Resolution** is a technique to identify data records that refer to the same real-world entity when unique identifiers are not available. It derives meaningful insights from data across the enterprise that reflects real-world entities and the relationships between them. Each indivudial record/document in a(or more) dataset(s) points to a single real-world entity.\n",
    "\n",
    "Entity resolution itself is known by multiple names: Data Deduplication, Record Linkage, Fuzzy Matching, Entity Matching and many more.\n",
    "\n",
    "### Steps Involved in Entity Resolution\n",
    "\n",
    "- **Data Preprocessing:** This involves all the steps involved in making data ready for the task.\n",
    "    - **Canonicalization:** Converting data with more than one possible representations into a standart form.\n",
    "    - **Data Cleaning:** Removing unnecessary contents in the data, that do not provide any relevant information.\n",
    "- **Blocking:** Group similar records together and separate them from other groups to reduce the number of comparisons. Blocks are created based on a number of blocking rules.\n",
    "- **Matching:** Once blocks are created, comparison is made between the records of the same block based on a set of matching rules defined.\n",
    "    - **Data Labelling:** Labelling random pairs of data if they represent a match or not. In our case we use *active learning* to label data. This facilitates supervised learning. In case of Unsupervised Entity Resolution this step is absent.\n",
    "    - **Featurization:** This steps involves in calculating a similarity score for each pair of records between the same block. It can be done in two ways:\n",
    "        - **Record-by-Record:** Concatenate all the fields together and calculate similarity score as a whole.\n",
    "        - **Field-by-Field:** Compare each attribute independently and add the scores together to get the final one. The advantage of using field-by-field over record-by-record is that we can assign different weights to each fields.\n",
    "    - **Classification:** Classifies similarity score obtained in the previous step to represent either distinct or duplicate records. Can also learn attribute weights and blocking rules.\n",
    "- **Clustering:** Grouping all the similar records together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "\n",
    "Data ingestion is the process of obtaining and importing data for immediate use or storage. The data used in this project can be obtained from [here](https://github.com/dedupeio/dedupe-examples/blob/master/csv_example/csv_example_messy_input.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import dedupe\n",
    "import pandas as pd\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate datasets and results files and folders according to your current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = './../data/er.csv'\n",
    "output_dir = './../results'\n",
    "output_file = './../results/er_output.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "- The data used in this session is a collection of all types of educational institutions located within United Stated of America.\n",
    "- There are 3681 rows and 32 columns in the dataset.\n",
    "- There are missing values in several columns with dominance in the later ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The `read_data()` function is used to read the csv data and returns it as a dictionary object. The `pre_process()` method is used to clean punctuations and irregular whitespaces from the data. The `unidecode()` method imported from unidecode library takes Unicode data and tries to represent it in ASCII characters (i.e., the universally displayable characters between 0x00 and 0x7F), where the compromises taken when mapping between two character sets are chosen to be near what a human with a US keyboard would choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = {}\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            clean_row = [(k, pre_process(v)) for (k, v) in row.items()]\n",
    "            row_id = int(row['Id'])\n",
    "            data[row_id] = dict(clean_row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(column):\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:   # Indicate missing value by None\n",
    "        column = None\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defing Blocking Rules\n",
    "\n",
    "Blocking Rules definition may vary according to the libraries we use. For this session, we will be working with [dedupe](https://dedupe.io/) which is a python open-source library for entity resolution. Dedupe difnes blocking rules as a dictionary where each key resembles the following:\n",
    "\n",
    "- **field:** attribute name\n",
    "- **type:** data type of the attribute value. This key is responsible to choose between different comparison parameters\n",
    "- **has_missing:** makes sure if the attribute has missing values\n",
    "\n",
    "For more detailed information about choosing the right **type** values refer to the [official documentation page](https://docs.dedupe.io/en/latest/Variable-definition.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    {'field': 'Site name', 'type': 'String'},\n",
    "    {'field': 'Address', 'type': 'String'},\n",
    "    {'field': 'Zip', 'type': 'Exact', 'has missing': True},\n",
    "    {'field': 'Phone', 'type': 'String', 'has missing': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dedupe object and pass blocking rules to it. The `prepare_training()` method is used to initialize active learning process according to the blocking rules fed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper = dedupe.Dedupe(fields)\n",
    "deduper.prepare_training(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning\n",
    "\n",
    "Active Learning is the method in which users are actively involved in labelling the data. Here `active` represents the real time interaction to label the data. The program itself displays a random pair of records each time and users have to label if the pairs are duplicates and based on this the program provides similarity score between other pairs of records of the same block to facilitate supervised learning. So in case of unsupervised learning, the process of active learning is absent. \n",
    "\n",
    "Dedupe provides a method `console_label()` to facilitate active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Site name : erie neighborhood house fcch-nury rodriguez\n",
      "Address : 2420 w lemoyne ave 1\n",
      "Zip : 60622\n",
      "Phone : None\n",
      "\n",
      "Site name : erie neighborhood house\n",
      "Address : 2510 w cortez street\n",
      "Zip : 60622\n",
      "Phone : 4867161\n",
      "\n",
      "14/10 positive, 14/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Site name : north avenue day nursery fcch-cheryl cook\n",
      "Address : 5929 w walton\n",
      "Zip : 60651\n",
      "Phone : None\n",
      "\n",
      "Site name : north avenue day nursery\n",
      "Address : 2001 w pierce street\n",
      "Zip : 60622\n",
      "Phone : 3424499\n",
      "\n",
      "14/10 positive, 15/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished / (p)revious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Site name : howard area community center - howard area community center\n",
      "Address : 7510 n ashland ave\n",
      "Zip : None\n",
      "Phone : 7647610\n",
      "\n",
      "Site name : howard area community services\n",
      "Address : 7610 n ashland avenue\n",
      "Zip : 60626\n",
      "Phone : 7647610\n",
      "\n",
      "14/10 positive, 16/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished / (p)revious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Site name : howard area community center - howard area community center\n",
      "Address : 7510 n ashland ave\n",
      "Zip : None\n",
      "Phone : 7647610\n",
      "\n",
      "Site name : howard area community services\n",
      "Address : 7610 n ashland avenue\n",
      "Zip : 60626\n",
      "Phone : 7647610\n",
      "\n",
      "15/10 positive, 16/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished / (p)revious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished labeling\n"
     ]
    }
   ],
   "source": [
    "dedupe.console_label(deduper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "We now have similarity score between each pairs of records that fall under the same block, but we still are not certain if the similarity score represents a match. We can use several classification techniques to classify the obtained similarity score to represent the pairs as duplicates or distinct entities.\n",
    "\n",
    "Sometimes this step is also used to monitor and learn blocking rules and attribute weights (in case of field-by-field comparison) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arun/Documents/NLP/env/lib/python3.8/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "deduper.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "So far we have been working with pairwise records only. Make resolution decision independently for each pair of records can be costly. So we apply different clustering algorithms to cluster all the similar records together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = deduper.partition(data, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "for cluster_id, (records, scores) in enumerate(clusters):\n",
    "    for record_id, score in zip(records, scores):\n",
    "        cluster_dict[record_id] = {\n",
    "            \"Cluster ID\": cluster_id,\n",
    "            \"confidence_score\": score\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results as csv\n",
    "\n",
    "We will be saving result obtained so far as csv file. The obtained csv file will have two additional columns relative to the original data: `Cluster ID` and `confidence_score`. `Cluster ID` is the id for each cluster of matched pairs while `confidence_score` is the certainty score for the given record to belong to given cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "with open(output_file, 'w') as f_output, open(input_file) as f_input:\n",
    "    reader = csv.DictReader(f_input)\n",
    "    fieldnames = ['Cluster ID', 'confidence_score'] + reader.fieldnames\n",
    "    writer = csv.DictWriter(f_output, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in reader:\n",
    "        row_id = int(row['Id'])\n",
    "        row.update(cluster_dict[row_id])\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python libraries that support Entity Resolution\n",
    "\n",
    "- [Splink](https://github.com/moj-analytical-services/splink) -> Unsupervised Learning\n",
    "- [Deepmatcher](https://github.com/anhaidgroup/deepmatcher) -> Supervised Learning\n",
    "- [FuzzyMatcher](https://github.com/RobinL/fuzzymatcher) -> Unsupervised Learning\n",
    "- [RLTK](https://github.com/usc-isi-i2/rltk) -> Supervised Learning\n",
    "- [RecordLinkage](https://github.com/J535D165/recordlinkage) -> Supervised/Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [An introduction to Entity Resolution — needs and challenges](https://towardsdatascience.com/an-introduction-to-entity-resolution-needs-and-challenges-97fba052dde5)\n",
    "- [End-to-End Entity Resolution for Big Data: A Survey](https://arxiv.org/pdf/1905.06397.pdf)\n",
    "- [Entity Resolution: Tutorial](http://home.cse.ust.hk/~leichen/courses/mscit6000d/notes/entityresolution.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
