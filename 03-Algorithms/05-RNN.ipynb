{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "Neural networks are a series of algorithms that mimic the operations of an animal brain to recognize relationships between vast amounts of data. Multilayer perceptrons are the simplest form of neural networks which consist of a input layer, one or more hidden layers and an output layer. A batch of input is passed at a time to the input layer and output is generated out of the output layer.  There are several types of neural networks, a few of them being Feed-forward neural networks, convolutional neural networks, deconvolutional neural networks, recurrent neural networks and modular neural networks.\n",
    "\n",
    "RNN is essentially repeating ANN designed to handle sequential data in which data is fed one at a time and the output of one serves as input to the next input in the sequence.\n",
    "\n",
    "## Why RNN?\n",
    "\n",
    "RNNs are required because basic Artificial Neural Networks:\n",
    "- cannot handle sequential data\n",
    "- considers only current input\n",
    "- cannot memorize previous inputs\n",
    "\n",
    "## Types of RNN\n",
    "\n",
    "- Single Layer Unidirectional RNN\n",
    "- Single Layer Bidirectional RNN\n",
    "- Multiple Layer Unidirectional RNN\n",
    "- Multiple Layer Bidirectional RNN\n",
    "\n",
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch import LongTensor\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['Recurrent', 'Neural', 'Network']   # Dummy data of batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', 'N', 'R', 'a', 'c', 'e', 'k', 'l', 'n', 'o', 'r', 't', 'u', 'w']\n",
      "Vocabulary Index: [[2, 5, 4, 12, 10, 10, 5, 8, 11], [1, 5, 12, 10, 3, 7], [1, 5, 11, 13, 9, 10, 6]]\n"
     ]
    }
   ],
   "source": [
    "vocab = [''] + sorted(set([char for seq in data for char in seq]))\n",
    "vocab2idx = [[vocab.index(token) for token in seq] for seq in data]\n",
    "print('Vocabulary:', vocab)\n",
    "print('Vocabulary Index:', vocab2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  5,  4, 12, 10, 10,  5,  8, 11],\n",
      "        [ 1,  5, 12, 10,  3,  7,  0,  0,  0],\n",
      "        [ 1,  5, 11, 13,  9, 10,  6,  0,  0]])\n",
      "tensor([[ 2,  1,  1],\n",
      "        [ 5,  5,  5],\n",
      "        [ 4, 12, 11],\n",
      "        [12, 10, 13],\n",
      "        [10,  3,  9],\n",
      "        [10,  7, 10],\n",
      "        [ 5,  0,  6],\n",
      "        [ 8,  0,  0],\n",
      "        [11,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = LongTensor([len(seq) for seq in vocab2idx])\n",
    "sequence_tensor = Variable(torch.zeros(len(vocab2idx), seq_lengths.max(), dtype=torch.long))\n",
    "\n",
    "for idx, (seq, seq_len) in enumerate(zip(vocab2idx, seq_lengths)):\n",
    "    sequence_tensor[idx, :seq_len] = LongTensor(seq)\n",
    "\n",
    "print(sequence_tensor)\n",
    "\n",
    "# convert the input into time major format (Transpose)\n",
    "sequence_tensor = sequence_tensor.t()    # Shape: (max_len, batch_size)\n",
    "print(sequence_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model Implementation\n",
    "\n",
    "Here we will define model classes inherited from pytorch's `nn.Module`. The `__init__()` method is used to instanciate each model classes while `forward()` method is used to define the model solution. The shape of different elements of `forward()` module are listed below:\n",
    "\n",
    "- *input* Shape: (max_len, batch_size)\n",
    "- *embed* Shape: (max_len, batch_size, embed_dim)\n",
    "- *hidden* Shape:\n",
    "    - Single Layer Unidirectional: (1, batch_size, hidden_size)\n",
    "    - Single Layer Bidirectional: (2, batch_size, hidden_size)\n",
    "    - Multiple Layer Unidirectional: (num_layers, batch_size, hidden_size)\n",
    "    - Multiple Layer Bidirectional: (num_layers * 2, batch_size, hidden_size)\n",
    "- *output* Shape: \n",
    "    - Unidirectional: (max_len, batch_size, hidden_size)\n",
    "    - Bidirectional: (max_len, batch_size, hidden_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "hidden_dim = 5\n",
    "embed_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, bidirectional):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embed = self.embedding(input)\n",
    "        output, hidden = self.rnn(embed)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Single Layer Unidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([9, 3])\n",
      "Output shape: torch.Size([9, 3, 5])\n",
      "Hidden shape: torch.Size([1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "n_layers = 1\n",
    "bidirectional = False\n",
    "\n",
    "model = RNNModel(input_dim, embed_dim, hidden_dim, n_layers, bidirectional)\n",
    "output, hidden = model(sequence_tensor)\n",
    "\n",
    "print(f\"Input shape: {sequence_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final output must be same as Hidden state in case of Single layer uni-directional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (output[-1, :, :] == hidden[0]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Single Layer Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([9, 3])\n",
      "Output shape: torch.Size([9, 3, 10])\n",
      "Hidden shape: torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "n_layers = 1\n",
    "bidirectional = True\n",
    "\n",
    "model = RNNModel(input_dim, embed_dim, hidden_dim, n_layers, bidirectional)\n",
    "output, hidden = model(sequence_tensor)\n",
    "\n",
    "print(f\"Input shape: {sequence_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First hidden_dim of output at last time step must be same as Final Forward Hidden state in case of Single layer bi-directional RNN\n",
    "- Last hidden_dim of output at initial time step must be same as Final Backward Hidden state in case of Single layer bi-directional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (output[-1, :, :hidden_dim] == hidden[0]).all()\n",
    "assert (output[0, :, hidden_dim:] == hidden[-1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Multiple Layer Unidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([9, 3])\n",
      "Output shape: torch.Size([9, 3, 5])\n",
      "Hidden shape: torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "n_layers = 2\n",
    "bidirectional = False\n",
    "\n",
    "model = RNNModel(input_dim, embed_dim, hidden_dim, n_layers, bidirectional)\n",
    "output, hidden = model(sequence_tensor)\n",
    "\n",
    "print(f\"Input shape: {sequence_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final output must be same as Final Hidden state in case of Multi layer uni-directional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (output[-1, :, :] == hidden[-1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Multiple Layer Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([9, 3])\n",
      "Output shape: torch.Size([9, 3, 10])\n",
      "Hidden shape: torch.Size([4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "n_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "model = RNNModel(input_dim, embed_dim, hidden_dim, n_layers, bidirectional)\n",
    "output, hidden = model(sequence_tensor)\n",
    "\n",
    "print(f\"Input shape: {sequence_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped hidden shape: torch.Size([2, 2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = sequence_tensor.shape[1]\n",
    "hidden = hidden.view(n_layers, 2, batch_size, hidden_dim)\n",
    "print(f\"Reshaped hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First hidden_dim of output at last time step must be same as Final Forward Hidden state of final layer in case of Multi layer bi-directional RNN\n",
    "- Last hidden_dim of output at initial time step must be same as Final Backward Hidden state of final layer in case of Multi layer bi-directional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (output[-1, :, :hidden_dim] == hidden[-1][0]).all()\n",
    "assert (output[0, :, hidden_dim:] == hidden[-1][1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Recurrent Neural Network Tutorial](https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn)\n",
    "- [Recurrent Neural Network Tutorial (RNN)](https://www.datacamp.com/tutorial/tutorial-for-recurrent-neural-network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
