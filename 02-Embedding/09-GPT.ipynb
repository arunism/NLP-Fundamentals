{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2\n",
    "\n",
    "GPT-2 is a large language model (LLM) developed by OpenAI. It is a powerful tool for natural language processing (NLP) tasks, such as text generation, machine translation, and question answering.\n",
    "\n",
    "GPT-2 is based on the transformer architecture, which is a neural network architecture that is particularly well-suited for NLP tasks. The transformer architecture consists of an encoder and a decoder. The encoder is responsible for encoding the input text into a representation that can be used by the decoder. The decoder then uses this representation to generate the output text.\n",
    "\n",
    "GPT-2 is trained on a massive dataset of text and code. The training process involves predicting the next word in a sequence of words. This is done using a technique called self-supervised learning, where the model is trained to predict the next word in a sequence of words, even when the words are randomly shuffled.\n",
    "\n",
    "The GPT-2 model has a number of advantages over other LLMs. First, it is very large, with 1.5 billion parameters. This allows it to learn more complex relationships between words. Second, it is trained on a massive dataset of text and code, which gives it a broader understanding of the world. Third, it is based on the transformer architecture, which is a very efficient architecture for NLP tasks.\n",
    "\n",
    "GPT-2 has been shown to be effective at a variety of NLP tasks. For example, it has been shown to be able to generate text that is indistinguishable from human-written text. It has also been shown to be able to translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
    "\n",
    "Here are some of the key features of the GPT-2 architecture:\n",
    "\n",
    "- **Self-attention:** GPT-2 uses self-attention to learn the relationships between words in a sequence. Self-attention is a technique that allows the model to attend to different parts of the input sequence, and to learn how these parts are related to each other.\n",
    "- **Transformer:** GPT-2 is based on the transformer architecture, which is a neural network architecture that is particularly well-suited for NLP tasks. The transformer architecture is efficient and scalable, and it allows the model to learn long-range dependencies between words.\n",
    "- **Massive dataset:** GPT-2 is trained on a massive dataset of text and code. This allows the model to learn a broad range of linguistic knowledge, and to generate text that is indistinguishable from human-written text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "text = \"this is a sentence\"\n",
    "# tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# model_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model_gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 6 tokens\n",
    "# encoded_input_bert = tokenizer_bert(text, return_tensors='pt')\n",
    "\n",
    "# 4 tokens\n",
    "encoded_input_gpt2 = tokenizer_gpt2(text, return_tensors='pt')\n",
    "\n",
    "# output_bert = model_bert(**encoded_input_bert)\n",
    "output_gpt2 = model_gpt2(**encoded_input_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is a simple example of how to use the Huggingface `transformers` library to tokenize and encode text using the GPT-2 model.\n",
    "\n",
    "The first line imports the `GPT2Tokenizer` and `GPT2Model` classes from the `transformers` library. These classes provide the functionality to tokenize text and to load and use the GPT-2 model.\n",
    "\n",
    "The next line defines the text that you want to tokenize and encode.\n",
    "\n",
    "The following lines create two tokenizers and two models, one for `BERT` and one for `GPT-2`. The tokenizers are used to tokenize the text, and the models are used to encode the tokenized text.\n",
    "\n",
    "The next two lines encode the text using the BERT and GPT-2 tokenizers, respectively. The encodings are returned as tensors, which are a type of data structure that is used by PyTorch.\n",
    "\n",
    "The final two lines use the BERT and GPT-2 models to generate outputs from the encodings. The outputs are also tensors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
