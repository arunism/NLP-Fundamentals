{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings from Language Model (ELMo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo is an state-of-the-art (SOTA) embedding technique developed by AllenNLP. The name ELMo resembles that the word embeddings are build over two-layer bidirectional language model (biLM).\n",
    "\n",
    "![ELMo](./../assets/embedding/emlo.png)\n",
    "\n",
    "*[[Image Source]](https://ireneli.eu/2018/12/17/elmo-in-practice/)*\n",
    "\n",
    "The two layers of biLM are stacked together and each layer has two moves: forward pass and backward pass.\n",
    "\n",
    "The biLM architecture uses character-level Convolutional Neural Network (CNN) to convert words into raw vectors which behave as input to first layer of biLM. The forward pass contains information about the current word and the context of the previous word. The backward pass holds the context and the information of the current word. The knowledge derived form both the passes aids in generating the intermediate word vectors which act as input to the next layer. Now the ultimate representation i.e. ELMo is the sum of one raw word vector and two intermediate word vectors.\n",
    "\n",
    "**Why ELMo?**\n",
    "- Since ELMo works on character-level, it can capture the internal structure of words i.e. grammar and semantics. For example:`code` and `coding` though both may not occur in the corpus, ELMo can figure out the relevance of one word with reference to another.\n",
    "\n",
    "- The vector corresponding to a word in the document is actually a function of the entire document. So the representation of the same word may differ from document to document based on the context. For example:\n",
    "\n",
    "    - He has been working at a coal mine since April.\n",
    "    - This is your bag, not mine.\n",
    "\n",
    "  The word `mine` in the first sentence is a noun while the same word `mine` in the second sentence is a pronoun (ploysemy phenomenon). ELMo generates different word embeddings for the same word in both the sentences.\n",
    "\n",
    "**Limitations**\n",
    "- Lacks arthematic relation between words (king + woman - man = queen)\n",
    "- Lacks cosine similarities between words (John â‰ˆ boy)\n",
    "\n",
    "To begin with, as always, let's start by installing the required libraires.\n",
    "\n",
    "```\n",
    "pip3 install allennlp\n",
    "pip3 install flair\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a9e37ffee3a5a10c5d687ae20c51ee3d4ba310e8eddff89fa7a028272d595a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
