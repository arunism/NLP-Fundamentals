{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformer (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT, as its name stands for, builds a bidirectional transformer-based language model using encoders rather than decoders. BERT is a state-of-the-art transformer architecture developed by `Google`. BERT is built on top of several popular ideas like [Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432), [ELMo](https://arxiv.org/abs/1802.05365), [ULM-FiT](https://arxiv.org/abs/1801.06146), [OpenAI Transformer](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) and [Transformer](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "The BERT architecture falls under an encoder-decoder(Transformer) model as follows:\n",
    "\n",
    "![BERT](../assets/embedding/bert.jpg)\n",
    "\n",
    "For fine-tuning and pre-training for different downstream tasks like Q/A, Classification, Language Modelling, Multiple Choice, NER etc. different layers of the BERT are used.\n",
    "\n",
    "**Why BERT?**\n",
    "- BERT can be used to extract features (word and sentence embeddings) from text. These features can be useful for keyword/search expansion, semantic search and information retrieval even if thereâ€™s no keyword or phrase overlap.\n",
    "- BERT can be used to fine-tune downstream models. The pretrained features can be used as high quality feature inputs to these downstream models.\n",
    "\n",
    "## Understanding BERT Input\n",
    "\n",
    "BERT uses two special tokens: `[CLS]` and `[SEP]`. The *[CLS]* token always appears at the beginning of the text and is specific to classification tasks. The *[SEP]* token is used to differentiate two different text documents (sentences). These two tokens are always required no matter what i.e. even if you have a single sentence or even if you are not training BERT for classification.\n",
    "\n",
    "```\n",
    "[CLS] Roses are red. [SEP] Sky is blue.\n",
    "[CLS] Mountains are earth's undecaying monuments. [SEP]\n",
    "```\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'mountains', 'are', 'earth', \"'\", 's', 'und', '##eca', '##ying', 'monuments', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Mountains are earth's undecaying monuments.\"\n",
    "marked_sentence = '[CLS] ' + sentence + ' [SEP]'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer.tokenize(marked_sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The word undecaying is represented as ['und', '##eca', '##ying']. The words that are not part of BERT vocabulary are represented as subwords and characters. The two preceding hashes are nothing but just a notation that the subword or the character is a part of larger group i.e. '##ying' is different from the independent 'ying' token.\n",
    "\n",
    "Now let's map the string tokens to vocabulary indeces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS]', 101), ('mountains', 4020), ('are', 2024), ('earth', 3011), (\"'\", 1005), ('s', 1055), ('und', 6151), ('##eca', 19281), ('##ying', 14147), ('monuments', 10490), ('.', 1012), ('[SEP]', 102)]\n"
     ]
    }
   ],
   "source": [
    "indeces = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(list(zip(tokens, indeces)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment ID\n",
    "\n",
    "Segment represents to each sentence in the text that are separated by *[SEP]* token. BERT needs segment ID to separate these segments: Segment 0 (series of 0s) and Segment 1 (series of 1s). In our case, we only need series of 1s as we are working with a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments = [1] * len(tokens)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Here we will be converting our indeces to tensors which serve as input to the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.tensor([indeces])\n",
    "segments = torch.tensor([segments])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using pre-trained model here. So, the model is subjected to evaluation mode by `model.eval()` and skip the training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bert-base-uncased` model has 12 layers. We will now run the text through BERT, and collect all of the hidden states produced by all 12 layers. \n",
    "\n",
    "`torch.no_grad()` prevents pytorch from constructing the computation graph during forward pass because it is of no use since we won't be backpropagating during evaluation mode. This reduces both time and storage complexities.\n",
    "\n",
    "Evaluating the model will return a different number of objects based on how it's  configured in the `from_pretrained` call earlier. In this case, becase we set `output_hidden_states=True`, the third item will be the hidden states from all layers. See the [documentation](https://huggingface.co/transformers/model_doc/bert.html#bertmodel) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens, segments)\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding BERT Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers: 13\n",
      "Number of Batches: 1\n",
      "Number of Tokens: 12\n",
      "Number of Hidden Units (Features): 768\n"
     ]
    }
   ],
   "source": [
    "layer_no = batch_no = token_no = 0\n",
    "\n",
    "print(f'Number of Layers: {len(hidden_states)}')\n",
    "print(f'Number of Batches: {len(hidden_states[layer_no])}')\n",
    "print(f'Number of Tokens: {len(hidden_states[layer_no][batch_no])}')\n",
    "print(f'Number of Hidden Units (Features): {len(hidden_states[layer_no][batch_no][token_no])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we get 13 layers though we earlier mentioned BERT model has 12 layers? It is because here first layer represents the input embeddings and the rest represents the outputs of each of the BERT's 12 layers. So, a total of 119808 (13x1x12x768) unique values are generated to represent our single sentence.\n",
    "\n",
    "```\n",
    "Current Dimensions Format: [# layers, # batches, # tokens, # features]\n",
    "Desired Dimensions Format: [# tokens, # layers, # features]\n",
    "```\n",
    "\n",
    "This can be achieved using PyTorch's `permute()` method which helps rearranging dimensions of a tensor. Let's begin by combining all the layers together to form one whole big tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings = torch.stack(hidden_states, dim=0)\n",
    "print(embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get rid of batches dimension as we do not need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings = torch.squeeze(embeddings, dim=1)\n",
    "print(embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's switch among the *layers* and *tokens* dimensions to get the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings = embeddings.permute(1, 0, 2)\n",
    "print(embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Till now each of our token has 13 separate vectors each of length 768. Now it's time we get indivudial vectors for each of our tokens. For this we need to pick some layers and combine respective vectors to obtain the final single vector. But how so we know, which combination gives the best result? Unfortunately, we don't know for sure. Let's try out a few approaches.\n",
    "\n",
    "#### I. Concatenate last four layers to get a single word vector per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 12 x 3072\n"
     ]
    }
   ],
   "source": [
    "single_vec = list()\n",
    "for token in embeddings:\n",
    "    vec_cat = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    single_vec.append(vec_cat)\n",
    "    \n",
    "print(f'Shape: {len(single_vec)} x {len(single_vec[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Summing the last four layers together to get a single word vector per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 12 x 768\n"
     ]
    }
   ],
   "source": [
    "single_vec = list()\n",
    "for token in embeddings:\n",
    "    vec_sum = torch.sum(token[-4:], dim=0)\n",
    "    single_vec.append(vec_sum)\n",
    "\n",
    "print(f'Shape: {len(single_vec)} x {len(single_vec[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embeddings\n",
    "\n",
    "Let's try getting single vector for the entire sentence but not just for a single word. For this we have multiple approaches that differ among different applications. Here we'll go with a simple strategy by averagin second to last hidden layer for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embedding shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "tokens_vec = hidden_states[-2][0]\n",
    "sentence_embedding = torch.mean(tokens_vec, dim=0)\n",
    "print(f'Sentence Embedding shape: {sentence_embedding.size()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[[Source]](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a9e37ffee3a5a10c5d687ae20c51ee3d4ba310e8eddff89fa7a028272d595a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
