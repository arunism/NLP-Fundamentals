{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Word2Vec deals with two different types of words to generate embedding vectors. The word we are looking into is the *Focus Word* and the words surrounding it are the *Context Words*. Word2Vec can be achieved using two methods: Skip-Gram and Common-Bag-of-Words (CBOW).\n",
    "\n",
    "`CBOW Model:` The core idea of CBOW is: Given a context word, can we predict the focus word? Let's understand the working of CBOW:\n",
    "1. Construct a vocabulary of size _v_.\n",
    "2. Represent each word using one-hot-encoding. So, each word corresponds to a *v*-dimensional binary vector.\n",
    "3. Pass v-dimensional context vectors as input to a neural network with N-dimensional hidden layer and generate an output vector again of v-dimensions.\n",
    "\n",
    "`Skip-Gram Model:` In skip-gram the behaviour gets flipped i.e. it predicts context words given the focus word. In both CBOW and skip-gram we have *k+1* numbers of *NxV* sized vectors. But CBOW has only 1 softmax to train while skip-gram has k softmax to train. So, it is obvious that skip-gram takes more time to train.\n",
    "\n",
    "In both cases the neurons of hidden layer consists of _linear_ activation function while the output layer is associated with *softmax* activation function. All the neurons are fully connected. It has been experienced that skip-gram works well with small amount of data and is found to represent rare words well while CBOW is faster and has better representations for more frequent words.\n",
    "\n",
    "![](./../assets/embedding/w2v.jpg)\n",
    "\n",
    "Word2Vec Embedding can be implemented using `gensim` library. gensim library has `Word2Vec` module that let's us work with the word2vec model easily. Let's begin by installing the required package:\n",
    "\n",
    "`pip3 install gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        sent = list()\n",
    "        for line in data.split('\\n\\n'):\n",
    "            sent.append(simple_preprocess(line))\n",
    "        return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences = read_file('./../data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building the Vocab\n",
    "\n",
    "`Word2Vec():` We initialize word2vec model with the argument *sentences* which is a list of sentences. The two parameters: *min_count* and *sample*, have a great influence over the performance of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences, min_count=20, sample=6e-5)\n",
    "model.save('./../models/w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exploring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('./../models/w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let see how our model lists the top n most similar words to the given input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lest', 0.9987840056419373),\n",
       " ('whom', 0.9987515211105347),\n",
       " ('clothes', 0.9987297654151917)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"smile\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see the analogy difference.\n",
    "\n",
    "`For example:` Which word is to woman as king is to queen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('humour', 0.9938604831695557),\n",
       " ('earth', 0.9937192797660828),\n",
       " ('cade', 0.9928563833236694)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['man', 'king'], negative=['queen'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check the similarity between two words i.e. how similar the words are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9858703"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('king', 'queen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's ask our model to separate a word that does not belong to the list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['king', 'queen', 'prince', 'walk'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
